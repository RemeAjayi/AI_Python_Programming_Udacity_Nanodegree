{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing an AI application\n",
    "\n",
    "Going forward, AI algorithms will be incorporated into more and more everyday applications. For example, you might want to include an image classifier in a smart phone app. To do this, you'd use a deep learning model trained on hundreds of thousands of images as part of the overall application architecture. A large part of software development in the future will be using these types of models as common parts of applications. \n",
    "\n",
    "In this project, you'll train an image classifier to recognize different species of flowers. You can imagine using something like this in a phone app that tells you the name of the flower your camera is looking at. In practice you'd train this classifier, then export it for use in your application. We'll be using [this dataset](http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html) of 102 flower categories, you can see a few examples below. \n",
    "\n",
    "<img src='assets/Flowers.png' width=500px>\n",
    "\n",
    "The project is broken down into multiple steps:\n",
    "\n",
    "* Load and preprocess the image dataset\n",
    "* Train the image classifier on your dataset\n",
    "* Use the trained classifier to predict image content\n",
    "\n",
    "We'll lead you through each part which you'll implement in Python.\n",
    "\n",
    "When you've completed this project, you'll have an application that can be trained on any set of labeled images. Here your network will be learning about flowers and end up as a command line application. But, what you do with your new skills depends on your imagination and effort in building a dataset. For example, imagine an app where you take a picture of a car, it tells you what the make and model is, then looks up information about it. Go build your own dataset and make something new.\n",
    "\n",
    "First up is importing the packages you'll need. It's good practice to keep all the imports at the beginning of your code. As you work through this notebook and find you need to import a package, make sure to add the import up here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import PIL\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import seaborn as sb\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from collections import OrderedDict \n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Here you'll use `torchvision` to load the data ([documentation](http://pytorch.org/docs/0.3.0/torchvision/index.html)). The data should be included alongside this notebook, otherwise you can [download it here](https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz). The dataset is split into three parts, training, validation, and testing. For the training, you'll want to apply transformations such as random scaling, cropping, and flipping. This will help the network generalize leading to better performance. You'll also need to make sure the input data is resized to 224x224 pixels as required by the pre-trained networks.\n",
    "\n",
    "The validation and testing sets are used to measure the model's performance on data it hasn't seen yet. For this you don't want any scaling or rotation transformations, but you'll need to resize then crop the images to the appropriate size.\n",
    "\n",
    "The pre-trained networks you'll use were trained on the ImageNet dataset where each color channel was normalized separately. For all three sets you'll need to normalize the means and standard deviations of the images to what the network expects. For the means, it's `[0.485, 0.456, 0.406]` and for the standard deviations `[0.229, 0.224, 0.225]`, calculated from the ImageNet images.  These values will shift each color channel to be centered at 0 and range from -1 to 1.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'flowers'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE: Define your transforms for the training, validation, and testing sets\n",
    "train_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                       transforms.RandomCrop(224),\n",
    "                                       transforms.RandomRotation(35),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.RandomVerticalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                          [0.229, 0.224, 0.225])\n",
    "                                      ])\n",
    "\n",
    "val_test_transforms = transforms.Compose([transforms.Resize(255), \n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                          [0.229, 0.224, 0.225])\n",
    "                                         ])\n",
    "\n",
    "# DONE: Load the datasets with ImageFolder\n",
    "train_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "valid_data = datasets.ImageFolder(valid_dir, transform=val_test_transforms)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=val_test_transforms)\n",
    "\n",
    "# DONE: Using the image datasets and the transforms, define the dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(valid_data, batch_size=32)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label mapping\n",
    "\n",
    "You'll also need to load in a mapping from category label to category name. You can find this in the file `cat_to_name.json`. It's a JSON object which you can read in with the [`json` module](https://docs.python.org/2/library/json.html). This will give you a dictionary mapping the integer encoded categories to the actual names of the flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'21': 'fire lily',\n",
       " '3': 'canterbury bells',\n",
       " '45': 'bolero deep blue',\n",
       " '1': 'pink primrose',\n",
       " '34': 'mexican aster',\n",
       " '27': 'prince of wales feathers',\n",
       " '7': 'moon orchid',\n",
       " '16': 'globe-flower',\n",
       " '25': 'grape hyacinth',\n",
       " '26': 'corn poppy',\n",
       " '79': 'toad lily',\n",
       " '39': 'siam tulip',\n",
       " '24': 'red ginger',\n",
       " '67': 'spring crocus',\n",
       " '35': 'alpine sea holly',\n",
       " '32': 'garden phlox',\n",
       " '10': 'globe thistle',\n",
       " '6': 'tiger lily',\n",
       " '93': 'ball moss',\n",
       " '33': 'love in the mist',\n",
       " '9': 'monkshood',\n",
       " '102': 'blackberry lily',\n",
       " '14': 'spear thistle',\n",
       " '19': 'balloon flower',\n",
       " '100': 'blanket flower',\n",
       " '13': 'king protea',\n",
       " '49': 'oxeye daisy',\n",
       " '15': 'yellow iris',\n",
       " '61': 'cautleya spicata',\n",
       " '31': 'carnation',\n",
       " '64': 'silverbush',\n",
       " '68': 'bearded iris',\n",
       " '63': 'black-eyed susan',\n",
       " '69': 'windflower',\n",
       " '62': 'japanese anemone',\n",
       " '20': 'giant white arum lily',\n",
       " '38': 'great masterwort',\n",
       " '4': 'sweet pea',\n",
       " '86': 'tree mallow',\n",
       " '101': 'trumpet creeper',\n",
       " '42': 'daffodil',\n",
       " '22': 'pincushion flower',\n",
       " '2': 'hard-leaved pocket orchid',\n",
       " '54': 'sunflower',\n",
       " '66': 'osteospermum',\n",
       " '70': 'tree poppy',\n",
       " '85': 'desert-rose',\n",
       " '99': 'bromelia',\n",
       " '87': 'magnolia',\n",
       " '5': 'english marigold',\n",
       " '92': 'bee balm',\n",
       " '28': 'stemless gentian',\n",
       " '97': 'mallow',\n",
       " '57': 'gaura',\n",
       " '40': 'lenten rose',\n",
       " '47': 'marigold',\n",
       " '59': 'orange dahlia',\n",
       " '48': 'buttercup',\n",
       " '55': 'pelargonium',\n",
       " '36': 'ruby-lipped cattleya',\n",
       " '91': 'hippeastrum',\n",
       " '29': 'artichoke',\n",
       " '71': 'gazania',\n",
       " '90': 'canna lily',\n",
       " '18': 'peruvian lily',\n",
       " '98': 'mexican petunia',\n",
       " '8': 'bird of paradise',\n",
       " '30': 'sweet william',\n",
       " '17': 'purple coneflower',\n",
       " '52': 'wild pansy',\n",
       " '84': 'columbine',\n",
       " '12': \"colt's foot\",\n",
       " '11': 'snapdragon',\n",
       " '96': 'camellia',\n",
       " '23': 'fritillary',\n",
       " '50': 'common dandelion',\n",
       " '44': 'poinsettia',\n",
       " '53': 'primula',\n",
       " '72': 'azalea',\n",
       " '65': 'californian poppy',\n",
       " '80': 'anthurium',\n",
       " '76': 'morning glory',\n",
       " '37': 'cape flower',\n",
       " '56': 'bishop of llandaff',\n",
       " '60': 'pink-yellow dahlia',\n",
       " '82': 'clematis',\n",
       " '58': 'geranium',\n",
       " '75': 'thorn apple',\n",
       " '41': 'barbeton daisy',\n",
       " '95': 'bougainvillea',\n",
       " '43': 'sword lily',\n",
       " '83': 'hibiscus',\n",
       " '78': 'lotus lotus',\n",
       " '88': 'cyclamen',\n",
       " '94': 'foxglove',\n",
       " '81': 'frangipani',\n",
       " '74': 'rose',\n",
       " '89': 'watercress',\n",
       " '73': 'water lily',\n",
       " '46': 'wallflower',\n",
       " '77': 'passion flower',\n",
       " '51': 'petunia'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)\n",
    "    \n",
    "cat_to_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and training the classifier\n",
    "\n",
    "Now that the data is ready, it's time to build and train the classifier. As usual, you should use one of the pretrained models from `torchvision.models` to get the image features. Build and train a new feed-forward classifier using those features.\n",
    "\n",
    "We're going to leave this part up to you. If you want to talk through it with someone, chat with your fellow students! You can also ask questions on the forums or join the instructors in office hours.\n",
    "\n",
    "Refer to [the rubric](https://review.udacity.com/#!/rubrics/1663/view) for guidance on successfully completing this section. Things you'll need to do:\n",
    "\n",
    "* Load a [pre-trained network](http://pytorch.org/docs/master/torchvision/models.html) (If you need a starting point, the VGG networks work great and are straightforward to use)\n",
    "* Define a new, untrained feed-forward network as a classifier, using ReLU activations and dropout\n",
    "* Train the classifier layers using backpropagation using the pre-trained network to get the features\n",
    "* Track the loss and accuracy on the validation set to determine the best hyperparameters\n",
    "\n",
    "We've left a cell open for you below, but use as many as you need. Our advice is to break the problem up into smaller parts you can run separately. Check that each part is doing what you expect, then move on to the next. You'll likely find that as you work through each part, you'll need to go back and modify your previous code. This is totally normal!\n",
    "\n",
    "When training make sure you're updating only the weights of the feed-forward network. You should be able to get the validation accuracy above 70% if you build everything right. Make sure to try different hyperparameters (learning rate, units in the classifier, epochs, etc) to find the best model. Save those hyperparameters to use as default values in the next part of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /home/ec2-user/.torch/models/resnet50-19c8e357.pth\n",
      "100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DONE: Build and train your network\n",
    "model = models.resnet50(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc2): Linear(in_features=512, out_features=102, bias=True)\n",
       "  (output): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Iterates through the entire pre-trained network and turns off the gradient calculation and update, effectively freezing\n",
    "#the network that has already been trained on ImageNet \n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = False\n",
    "\n",
    "#this adapts and tunes the head classifier 'fc' to fit this flower dataset with only 102 flower species, instead of 1000 classes\n",
    "#this part of the model will be trained on the flower dataset\n",
    "model.fc = nn.Sequential(OrderedDict([\n",
    "                                    ('fc1', nn.Linear(2048, 512)),\n",
    "                                    ('relu', nn.ReLU()),\n",
    "                                    ('dropout', nn.Dropout(p=0.5)),\n",
    "                                    ('fc2', nn.Linear(512, 102)),\n",
    "                                    #the logarithm of the softmax probabilities is returned because of faster and \n",
    "                                    #usually more accurate computation. As computers use a base 2 number system, the computer\n",
    "                                    #would store the float values of the softmax probabilities with imprecision\n",
    "                                    #logarithm of the softmax improves numerical stability\n",
    "                                    ('output', nn.LogSoftmax(dim=1))\n",
    "                                    ]))\n",
    "\n",
    "model.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation function is largely taken from this udacity tutorial: https://classroom.udacity.com/nanodegrees/nd089/parts/4f56c240-e23f-49ae-9b84-b84c980fad9c/modules/47a93218-8271-457b-aca6-e2cdca10131b/lessons/e1eeafe1-2ba0-4f3d-97a0-82cbd844fdfc/concepts/43cb782f-2d8c-432e-94ef-cb8068f26042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE: Do validation on the validation set\n",
    "def validation(loader, device='cpu'):\n",
    "    \n",
    "    model.to(device) #sends to the network to either cpu ('cpu') or gpu ('cuda')\n",
    "    model.eval() #turns dropout off to give an undistorted validation and test result\n",
    "    \n",
    "    #we use Negative Likelihood log loss function because the output from the model is log_Softmax\n",
    "    criterion = nn.NLLLoss() \n",
    "    running_loss = 0 #initialization of loss score\n",
    "    acc = 0 #initialization of accuracy \n",
    "    \n",
    "    #the .no_grad ensures that the model does not calculate the gradient and update the weights during validation and testing\n",
    "    with torch.no_grad():\n",
    "        #the for loop to iterate over all images and labels in the validation and test sets \n",
    "        for images, labels in loader:\n",
    "            #this sends the data input to either cpu ('cpu') or gpu ('cuda')\n",
    "            images, labels = images.to(device), labels.to(device) \n",
    "            \n",
    "            #forward pass and only forward pass\n",
    "            output = model.forward(images)\n",
    "            running_loss += criterion(output, labels).item()\n",
    "\n",
    "            probs = torch.exp(output)\n",
    "            \n",
    "            #this is the comparison of the truth class index with the predicted class index to get the corrected prediciton\n",
    "            #that is a list of 0s and 1s. \n",
    "            corrected_pred = (labels.data == probs.max(dim=1)[1])\n",
    "            \n",
    "            #here we get the accuracy by first converting the tensor a float tensor and apply the mean method\n",
    "            #which sums up all the 1s and 0s, and divides by the total number of items\n",
    "            acc += corrected_pred.type(torch.FloatTensor).mean()\n",
    "    \n",
    "    return running_loss/len(loader), acc/len(loader) * 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, optimizer, device='cpu'):   \n",
    "    \n",
    "    epochs = epochs \n",
    "    \n",
    "    steps = 0 \n",
    "    running_loss = 0\n",
    "    acc = 0 \n",
    "    \n",
    "    model.to(device)#this gives the option of either cpu('cpu') or gpu('cuda')\n",
    "    \n",
    "    #Negative Log Likelihood Loss function, we use this because the output is in log form \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        for idx, (images, labels) in enumerate(trainloader):\n",
    "            \n",
    "            steps += 1\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            #sets the gradient to zero after each pass to prevent accumulating an inaccurately large gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model.forward(images)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            #Backward pass: where backward propagation occurs \n",
    "            loss.backward()\n",
    "            \n",
    "            #the weight update step where the optimizer applies the calculated negative gradient to weights and biases\n",
    "            optimizer.step()\n",
    "            \n",
    "            #captures the loss during training, getting the scalar value from the tensor utilizing .item() method \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #this converts the log ouput back to expected list of probability values between 0 and 1 using exponential function\n",
    "            #the probs also contains a list of the corresponding class index second in the index at ps[1]\n",
    "            probs = torch.exp(output)\n",
    "            \n",
    "            #this compares the correct class index (label.data) with the predicted class index (probs.max(dim=1)[1]) \n",
    "                                                                                    #associated with max probability\n",
    "            corrected_pred = (labels.data == probs.max(dim=1)[1])\n",
    "            \n",
    "            #this captures the accuracy by taking the mean of all the comparisons that are either 1 for correct \n",
    "            #and 0 for not correct. The eq value is converted to a float tensor because it has the mean as a method \n",
    "            acc += corrected_pred.type(torch.FloatTensor).mean()\n",
    "\n",
    "            if steps % 40 == 0:\n",
    "                \n",
    "                val_loss, val_acc = validation(validloader, 'cuda')\n",
    "                \n",
    "                print(                    #add 1 to e because range(epochs) starts at 0\n",
    "                     'Epoch {}/{}'.format(e+1, epochs), \n",
    "                     'training loss: {:.4f}'.format(running_loss/40), \n",
    "                     'training accuracy: {:.4f}  '.format(acc/40 * 100),\n",
    "                     #do not need to divide by length of validation set, already done in the function\n",
    "                     'Validation loss: {:.4f}'.format(val_loss), \n",
    "                     'Validation accuracy: {}%'.format(val_acc)\n",
    "                     )\n",
    "                #sets the running_loss and accuracy back to zero so the loss and accuracy do not accumulate\n",
    "                #and become too large, distorting the performance view during training\n",
    "                running_loss = 0\n",
    "                acc = 0 \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here was use the Adam optimizer with a learning rate of .001, \n",
    "#applies the Adam optimizer to only the head classifier of the pretrained network\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr = .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35 training loss: 4.5249 training accuracy: 6.3281   Validation loss: 4.2181 Validation accuracy: 11.658653259277344%\n",
      "Epoch 1/35 training loss: 3.9424 training accuracy: 16.7969   Validation loss: 3.5939 Validation accuracy: 22.836538314819336%\n",
      "Epoch 1/35 training loss: 3.2451 training accuracy: 32.5781   Validation loss: 2.7478 Validation accuracy: 39.0625%\n",
      "Epoch 1/35 training loss: 2.4814 training accuracy: 44.2188   Validation loss: 2.2213 Validation accuracy: 46.501068115234375%\n",
      "Epoch 1/35 training loss: 1.9162 training accuracy: 53.9844   Validation loss: 1.6644 Validation accuracy: 61.15117645263672%\n",
      "Epoch 2/35 training loss: 1.4829 training accuracy: 64.7396   Validation loss: 1.4291 Validation accuracy: 61.61859130859375%\n",
      "Epoch 2/35 training loss: 1.3169 training accuracy: 67.5781   Validation loss: 1.2752 Validation accuracy: 67.52136993408203%\n",
      "Epoch 2/35 training loss: 1.1431 training accuracy: 72.1875   Validation loss: 1.2116 Validation accuracy: 69.8851547241211%\n",
      "Epoch 2/35 training loss: 1.0936 training accuracy: 71.6406   Validation loss: 1.0489 Validation accuracy: 72.99679565429688%\n",
      "Epoch 2/35 training loss: 0.8991 training accuracy: 77.3438   Validation loss: 0.9802 Validation accuracy: 74.89315795898438%\n",
      "Epoch 3/35 training loss: 0.8726 training accuracy: 77.2656   Validation loss: 0.9178 Validation accuracy: 75.82799530029297%\n",
      "Epoch 3/35 training loss: 0.7558 training accuracy: 81.3281   Validation loss: 0.7855 Validation accuracy: 81.03632354736328%\n",
      "Epoch 3/35 training loss: 0.7201 training accuracy: 80.9375   Validation loss: 0.7471 Validation accuracy: 81.383544921875%\n",
      "Epoch 3/35 training loss: 0.6887 training accuracy: 81.4062   Validation loss: 0.8005 Validation accuracy: 77.41719818115234%\n",
      "Epoch 3/35 training loss: 0.6626 training accuracy: 82.4219   Validation loss: 0.7150 Validation accuracy: 81.39690399169922%\n",
      "Epoch 4/35 training loss: 0.6455 training accuracy: 82.3177   Validation loss: 0.6339 Validation accuracy: 84.04113006591797%\n",
      "Epoch 4/35 training loss: 0.6083 training accuracy: 82.2656   Validation loss: 0.6783 Validation accuracy: 81.43695831298828%\n",
      "Epoch 4/35 training loss: 0.5630 training accuracy: 84.3750   Validation loss: 0.7675 Validation accuracy: 78.0849380493164%\n",
      "Epoch 4/35 training loss: 0.5649 training accuracy: 83.6719   Validation loss: 0.7283 Validation accuracy: 79.9813003540039%\n",
      "Epoch 4/35 training loss: 0.5759 training accuracy: 84.1406   Validation loss: 0.6912 Validation accuracy: 80.70246124267578%\n",
      "Epoch 5/35 training loss: 0.5169 training accuracy: 85.5990   Validation loss: 0.6288 Validation accuracy: 80.46207427978516%\n",
      "Epoch 5/35 training loss: 0.4908 training accuracy: 86.4062   Validation loss: 0.7128 Validation accuracy: 81.16986846923828%\n",
      "Epoch 5/35 training loss: 0.4954 training accuracy: 86.6406   Validation loss: 0.5771 Validation accuracy: 83.93429565429688%\n",
      "Epoch 5/35 training loss: 0.4626 training accuracy: 86.9531   Validation loss: 0.5947 Validation accuracy: 83.22649383544922%\n",
      "Epoch 5/35 training loss: 0.4710 training accuracy: 85.9375   Validation loss: 0.5432 Validation accuracy: 84.88247680664062%\n",
      "Epoch 6/35 training loss: 0.4371 training accuracy: 87.7865   Validation loss: 0.5381 Validation accuracy: 85.72382354736328%\n",
      "Epoch 6/35 training loss: 0.3980 training accuracy: 88.2812   Validation loss: 0.6206 Validation accuracy: 82.95940399169922%\n",
      "Epoch 6/35 training loss: 0.4295 training accuracy: 87.4219   Validation loss: 0.6151 Validation accuracy: 81.86431884765625%\n",
      "Epoch 6/35 training loss: 0.4184 training accuracy: 87.5781   Validation loss: 0.5077 Validation accuracy: 86.83226776123047%\n",
      "Epoch 6/35 training loss: 0.4835 training accuracy: 85.7812   Validation loss: 0.5075 Validation accuracy: 86.071044921875%\n",
      "Epoch 7/35 training loss: 0.4111 training accuracy: 88.3594   Validation loss: 0.5033 Validation accuracy: 85.99092102050781%\n",
      "Epoch 7/35 training loss: 0.3610 training accuracy: 90.4688   Validation loss: 0.5262 Validation accuracy: 85.536865234375%\n",
      "Epoch 7/35 training loss: 0.3797 training accuracy: 89.2969   Validation loss: 0.5292 Validation accuracy: 85.60363006591797%\n",
      "Epoch 7/35 training loss: 0.3898 training accuracy: 88.9062   Validation loss: 0.5473 Validation accuracy: 85.5101547241211%\n",
      "Epoch 7/35 training loss: 0.3498 training accuracy: 90.2344   Validation loss: 0.5181 Validation accuracy: 85.9642105102539%\n",
      "Epoch 8/35 training loss: 0.3911 training accuracy: 87.8646   Validation loss: 0.4871 Validation accuracy: 86.83226776123047%\n",
      "Epoch 8/35 training loss: 0.4232 training accuracy: 87.6562   Validation loss: 0.5389 Validation accuracy: 86.68536376953125%\n",
      "Epoch 8/35 training loss: 0.3285 training accuracy: 90.8594   Validation loss: 0.4740 Validation accuracy: 87.5267105102539%\n",
      "Epoch 8/35 training loss: 0.3352 training accuracy: 90.3125   Validation loss: 0.4610 Validation accuracy: 86.79219818115234%\n",
      "Epoch 8/35 training loss: 0.3486 training accuracy: 89.9219   Validation loss: 0.4613 Validation accuracy: 87.28632354736328%\n",
      "Epoch 8/35 training loss: 0.3223 training accuracy: 91.2240   Validation loss: 0.5254 Validation accuracy: 86.1111068725586%\n",
      "Epoch 9/35 training loss: 0.3222 training accuracy: 90.6250   Validation loss: 0.5227 Validation accuracy: 86.44497680664062%\n",
      "Epoch 9/35 training loss: 0.3410 training accuracy: 90.2344   Validation loss: 0.4991 Validation accuracy: 84.6688003540039%\n",
      "Epoch 9/35 training loss: 0.3450 training accuracy: 88.5156   Validation loss: 0.4866 Validation accuracy: 85.29647827148438%\n",
      "Epoch 9/35 training loss: 0.3682 training accuracy: 89.7656   Validation loss: 0.5365 Validation accuracy: 85.5101547241211%\n",
      "Epoch 9/35 training loss: 0.3672 training accuracy: 89.2188   Validation loss: 0.4731 Validation accuracy: 86.92575073242188%\n",
      "Epoch 10/35 training loss: 0.3040 training accuracy: 90.8594   Validation loss: 0.4890 Validation accuracy: 86.44497680664062%\n",
      "Epoch 10/35 training loss: 0.3039 training accuracy: 91.4062   Validation loss: 0.4878 Validation accuracy: 86.56517028808594%\n",
      "Epoch 10/35 training loss: 0.3900 training accuracy: 88.2812   Validation loss: 0.4498 Validation accuracy: 88.24786376953125%\n",
      "Epoch 10/35 training loss: 0.3066 training accuracy: 90.7031   Validation loss: 0.4694 Validation accuracy: 86.1111068725586%\n",
      "Epoch 10/35 training loss: 0.3217 training accuracy: 89.3750   Validation loss: 0.4975 Validation accuracy: 86.20459747314453%\n",
      "Epoch 11/35 training loss: 0.2712 training accuracy: 90.8854   Validation loss: 0.5228 Validation accuracy: 85.60363006591797%\n",
      "Epoch 11/35 training loss: 0.3114 training accuracy: 90.7812   Validation loss: 0.4631 Validation accuracy: 87.76709747314453%\n",
      "Epoch 11/35 training loss: 0.3011 training accuracy: 90.7031   Validation loss: 0.4335 Validation accuracy: 88.72863006591797%\n",
      "Epoch 11/35 training loss: 0.2806 training accuracy: 92.0312   Validation loss: 0.4139 Validation accuracy: 88.60844421386719%\n",
      "Epoch 11/35 training loss: 0.3102 training accuracy: 91.0938   Validation loss: 0.5899 Validation accuracy: 85.02938079833984%\n",
      "Epoch 12/35 training loss: 0.2860 training accuracy: 91.7448   Validation loss: 0.4388 Validation accuracy: 89.32959747314453%\n",
      "Epoch 12/35 training loss: 0.2430 training accuracy: 93.4375   Validation loss: 0.4844 Validation accuracy: 88.00747680664062%\n",
      "Epoch 12/35 training loss: 0.2543 training accuracy: 91.7188   Validation loss: 0.5185 Validation accuracy: 85.37660217285156%\n",
      "Epoch 12/35 training loss: 0.2662 training accuracy: 91.6406   Validation loss: 0.5232 Validation accuracy: 86.20459747314453%\n",
      "Epoch 12/35 training loss: 0.2745 training accuracy: 91.4844   Validation loss: 0.4212 Validation accuracy: 89.20940399169922%\n",
      "Epoch 13/35 training loss: 0.3098 training accuracy: 90.8594   Validation loss: 0.5089 Validation accuracy: 87.28632354736328%\n",
      "Epoch 13/35 training loss: 0.2962 training accuracy: 91.0938   Validation loss: 0.4776 Validation accuracy: 87.00587463378906%\n",
      "Epoch 13/35 training loss: 0.2766 training accuracy: 91.2500   Validation loss: 0.4827 Validation accuracy: 88.00747680664062%\n",
      "Epoch 13/35 training loss: 0.3200 training accuracy: 90.3906   Validation loss: 0.5300 Validation accuracy: 86.20459747314453%\n",
      "Epoch 13/35 training loss: 0.2924 training accuracy: 91.3281   Validation loss: 0.4733 Validation accuracy: 87.94070434570312%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/35 training loss: 0.2312 training accuracy: 92.6302   Validation loss: 0.4698 Validation accuracy: 87.64690399169922%\n",
      "Epoch 14/35 training loss: 0.2687 training accuracy: 91.7188   Validation loss: 0.4248 Validation accuracy: 88.87554168701172%\n",
      "Epoch 14/35 training loss: 0.2192 training accuracy: 93.4375   Validation loss: 0.5016 Validation accuracy: 87.31304168701172%\n",
      "Epoch 14/35 training loss: 0.2694 training accuracy: 92.1094   Validation loss: 0.4827 Validation accuracy: 87.76709747314453%\n",
      "Epoch 14/35 training loss: 0.2817 training accuracy: 91.8750   Validation loss: 0.3917 Validation accuracy: 88.7553482055664%\n",
      "Epoch 15/35 training loss: 0.2499 training accuracy: 92.6302   Validation loss: 0.4870 Validation accuracy: 87.04594421386719%\n",
      "Epoch 15/35 training loss: 0.2644 training accuracy: 91.4844   Validation loss: 0.4210 Validation accuracy: 90.17094421386719%\n",
      "Epoch 15/35 training loss: 0.2354 training accuracy: 93.1250   Validation loss: 0.4287 Validation accuracy: 88.9957275390625%\n",
      "Epoch 15/35 training loss: 0.2378 training accuracy: 93.0469   Validation loss: 0.4431 Validation accuracy: 87.5267105102539%\n",
      "Epoch 15/35 training loss: 0.2593 training accuracy: 91.9531   Validation loss: 0.4891 Validation accuracy: 86.71207427978516%\n",
      "Epoch 16/35 training loss: 0.2579 training accuracy: 92.0052   Validation loss: 0.4857 Validation accuracy: 87.5267105102539%\n",
      "Epoch 16/35 training loss: 0.2157 training accuracy: 93.2812   Validation loss: 0.4151 Validation accuracy: 89.44978332519531%\n",
      "Epoch 16/35 training loss: 0.2277 training accuracy: 92.5000   Validation loss: 0.4429 Validation accuracy: 87.5267105102539%\n",
      "Epoch 16/35 training loss: 0.2279 training accuracy: 93.0469   Validation loss: 0.4041 Validation accuracy: 89.69017028808594%\n",
      "Epoch 16/35 training loss: 0.2309 training accuracy: 93.0469   Validation loss: 0.3706 Validation accuracy: 90.5315170288086%\n",
      "Epoch 16/35 training loss: 0.2306 training accuracy: 92.7865   Validation loss: 0.3988 Validation accuracy: 89.44978332519531%\n",
      "Epoch 17/35 training loss: 0.2196 training accuracy: 93.2812   Validation loss: 0.4236 Validation accuracy: 88.84882354736328%\n",
      "Epoch 17/35 training loss: 0.2560 training accuracy: 91.7969   Validation loss: 0.4705 Validation accuracy: 86.92575073242188%\n",
      "Epoch 17/35 training loss: 0.2555 training accuracy: 92.2656   Validation loss: 0.4215 Validation accuracy: 88.7553482055664%\n",
      "Epoch 17/35 training loss: 0.2405 training accuracy: 91.7969   Validation loss: 0.3868 Validation accuracy: 90.41132354736328%\n",
      "Epoch 17/35 training loss: 0.2644 training accuracy: 91.2500   Validation loss: 0.4742 Validation accuracy: 87.76709747314453%\n",
      "Epoch 18/35 training loss: 0.2428 training accuracy: 92.0312   Validation loss: 0.3931 Validation accuracy: 91.01228332519531%\n",
      "Epoch 18/35 training loss: 0.2257 training accuracy: 93.0469   Validation loss: 0.4939 Validation accuracy: 87.41986846923828%\n",
      "Epoch 18/35 training loss: 0.2400 training accuracy: 92.6562   Validation loss: 0.4840 Validation accuracy: 87.4065170288086%\n",
      "Epoch 18/35 training loss: 0.2065 training accuracy: 94.1406   Validation loss: 0.4188 Validation accuracy: 89.0892105102539%\n",
      "Epoch 18/35 training loss: 0.2130 training accuracy: 92.9688   Validation loss: 0.4026 Validation accuracy: 89.69017028808594%\n",
      "Epoch 19/35 training loss: 0.2089 training accuracy: 93.7500   Validation loss: 0.4030 Validation accuracy: 88.72863006591797%\n",
      "Epoch 19/35 training loss: 0.2276 training accuracy: 91.9531   Validation loss: 0.5037 Validation accuracy: 85.56356811523438%\n",
      "Epoch 19/35 training loss: 0.2370 training accuracy: 92.1094   Validation loss: 0.4556 Validation accuracy: 88.36805725097656%\n",
      "Epoch 19/35 training loss: 0.2297 training accuracy: 92.8125   Validation loss: 0.4597 Validation accuracy: 89.38301849365234%\n",
      "Epoch 19/35 training loss: 0.2359 training accuracy: 92.7344   Validation loss: 0.4660 Validation accuracy: 87.28632354736328%\n",
      "Epoch 20/35 training loss: 0.2066 training accuracy: 93.1771   Validation loss: 0.4062 Validation accuracy: 89.32959747314453%\n",
      "Epoch 20/35 training loss: 0.1695 training accuracy: 94.4531   Validation loss: 0.5192 Validation accuracy: 86.45832824707031%\n",
      "Epoch 20/35 training loss: 0.2290 training accuracy: 92.5781   Validation loss: 0.4387 Validation accuracy: 88.72863006591797%\n",
      "Epoch 20/35 training loss: 0.2522 training accuracy: 91.8750   Validation loss: 0.4669 Validation accuracy: 88.39476776123047%\n",
      "Epoch 20/35 training loss: 0.2669 training accuracy: 91.4844   Validation loss: 0.4856 Validation accuracy: 87.76709747314453%\n",
      "Epoch 21/35 training loss: 0.2287 training accuracy: 93.2552   Validation loss: 0.4899 Validation accuracy: 86.56517028808594%\n",
      "Epoch 21/35 training loss: 0.2047 training accuracy: 93.2812   Validation loss: 0.4210 Validation accuracy: 88.12767028808594%\n",
      "Epoch 21/35 training loss: 0.1758 training accuracy: 94.6094   Validation loss: 0.4075 Validation accuracy: 89.0892105102539%\n",
      "Epoch 21/35 training loss: 0.2067 training accuracy: 92.9688   Validation loss: 0.4400 Validation accuracy: 88.03418731689453%\n",
      "Epoch 21/35 training loss: 0.1918 training accuracy: 93.7500   Validation loss: 0.4386 Validation accuracy: 86.69871520996094%\n",
      "Epoch 22/35 training loss: 0.1756 training accuracy: 94.5312   Validation loss: 0.4112 Validation accuracy: 90.224365234375%\n",
      "Epoch 22/35 training loss: 0.2150 training accuracy: 93.2031   Validation loss: 0.4193 Validation accuracy: 89.0892105102539%\n",
      "Epoch 22/35 training loss: 0.1890 training accuracy: 93.4375   Validation loss: 0.3733 Validation accuracy: 89.81036376953125%\n",
      "Epoch 22/35 training loss: 0.2228 training accuracy: 93.4375   Validation loss: 0.4078 Validation accuracy: 90.77190399169922%\n",
      "Epoch 22/35 training loss: 0.1883 training accuracy: 93.9844   Validation loss: 0.4466 Validation accuracy: 87.28632354736328%\n",
      "Epoch 23/35 training loss: 0.2321 training accuracy: 92.7865   Validation loss: 0.3988 Validation accuracy: 90.17094421386719%\n",
      "Epoch 23/35 training loss: 0.1667 training accuracy: 95.3906   Validation loss: 0.4299 Validation accuracy: 88.95565795898438%\n",
      "Epoch 23/35 training loss: 0.2056 training accuracy: 93.9062   Validation loss: 0.4330 Validation accuracy: 90.05075073242188%\n",
      "Epoch 23/35 training loss: 0.1910 training accuracy: 93.5156   Validation loss: 0.3871 Validation accuracy: 90.17094421386719%\n",
      "Epoch 23/35 training loss: 0.2089 training accuracy: 93.5156   Validation loss: 0.4601 Validation accuracy: 87.88728332519531%\n",
      "Epoch 24/35 training loss: 0.1953 training accuracy: 93.6719   Validation loss: 0.4286 Validation accuracy: 87.88728332519531%\n",
      "Epoch 24/35 training loss: 0.1989 training accuracy: 93.8281   Validation loss: 0.4539 Validation accuracy: 88.72863006591797%\n",
      "Epoch 24/35 training loss: 0.2476 training accuracy: 92.0312   Validation loss: 0.4449 Validation accuracy: 87.88728332519531%\n",
      "Epoch 24/35 training loss: 0.2212 training accuracy: 92.5781   Validation loss: 0.4632 Validation accuracy: 88.15438079833984%\n",
      "Epoch 24/35 training loss: 0.2159 training accuracy: 93.4375   Validation loss: 0.4237 Validation accuracy: 88.24786376953125%\n",
      "Epoch 24/35 training loss: 0.2444 training accuracy: 92.9948   Validation loss: 0.4884 Validation accuracy: 87.72703552246094%\n",
      "Epoch 25/35 training loss: 0.1999 training accuracy: 93.4375   Validation loss: 0.4342 Validation accuracy: 89.32959747314453%\n",
      "Epoch 25/35 training loss: 0.1927 training accuracy: 94.2188   Validation loss: 0.4706 Validation accuracy: 87.88728332519531%\n",
      "Epoch 25/35 training loss: 0.1859 training accuracy: 94.3750   Validation loss: 0.4925 Validation accuracy: 87.88728332519531%\n",
      "Epoch 25/35 training loss: 0.2066 training accuracy: 92.1875   Validation loss: 0.4459 Validation accuracy: 89.44978332519531%\n",
      "Epoch 25/35 training loss: 0.1956 training accuracy: 93.7500   Validation loss: 0.3926 Validation accuracy: 90.6517105102539%\n",
      "Epoch 26/35 training loss: 0.1450 training accuracy: 95.4167   Validation loss: 0.3986 Validation accuracy: 89.56997680664062%\n",
      "Epoch 26/35 training loss: 0.2092 training accuracy: 93.3594   Validation loss: 0.4537 Validation accuracy: 89.20940399169922%\n",
      "Epoch 26/35 training loss: 0.2163 training accuracy: 93.3594   Validation loss: 0.4138 Validation accuracy: 90.29113006591797%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/35 training loss: 0.2163 training accuracy: 93.2812   Validation loss: 0.4349 Validation accuracy: 88.78205108642578%\n",
      "Epoch 26/35 training loss: 0.2498 training accuracy: 91.9531   Validation loss: 0.4783 Validation accuracy: 87.76709747314453%\n",
      "Epoch 27/35 training loss: 0.1923 training accuracy: 93.8021   Validation loss: 0.3991 Validation accuracy: 88.84882354736328%\n",
      "Epoch 27/35 training loss: 0.2119 training accuracy: 93.4375   Validation loss: 0.4777 Validation accuracy: 88.30128479003906%\n",
      "Epoch 27/35 training loss: 0.1560 training accuracy: 94.6875   Validation loss: 0.4430 Validation accuracy: 89.62339782714844%\n",
      "Epoch 27/35 training loss: 0.1790 training accuracy: 93.8281   Validation loss: 0.4050 Validation accuracy: 89.69017028808594%\n",
      "Epoch 27/35 training loss: 0.2020 training accuracy: 93.7500   Validation loss: 0.5244 Validation accuracy: 86.80555725097656%\n",
      "Epoch 28/35 training loss: 0.2124 training accuracy: 92.7083   Validation loss: 0.4330 Validation accuracy: 88.87554168701172%\n",
      "Epoch 28/35 training loss: 0.2165 training accuracy: 93.8281   Validation loss: 0.4660 Validation accuracy: 87.70032501220703%\n",
      "Epoch 28/35 training loss: 0.2097 training accuracy: 93.0469   Validation loss: 0.4287 Validation accuracy: 89.0892105102539%\n",
      "Epoch 28/35 training loss: 0.1404 training accuracy: 95.4688   Validation loss: 0.4562 Validation accuracy: 88.9690170288086%\n",
      "Epoch 28/35 training loss: 0.1518 training accuracy: 95.3125   Validation loss: 0.3759 Validation accuracy: 89.81036376953125%\n",
      "Epoch 29/35 training loss: 0.1418 training accuracy: 95.4688   Validation loss: 0.4155 Validation accuracy: 89.74359130859375%\n",
      "Epoch 29/35 training loss: 0.2052 training accuracy: 93.2031   Validation loss: 0.5079 Validation accuracy: 87.4065170288086%\n",
      "Epoch 29/35 training loss: 0.1919 training accuracy: 93.9062   Validation loss: 0.4393 Validation accuracy: 90.41132354736328%\n",
      "Epoch 29/35 training loss: 0.1779 training accuracy: 94.0625   Validation loss: 0.3958 Validation accuracy: 89.69017028808594%\n",
      "Epoch 29/35 training loss: 0.1584 training accuracy: 94.8438   Validation loss: 0.4013 Validation accuracy: 89.43643188476562%\n",
      "Epoch 30/35 training loss: 0.1804 training accuracy: 94.8177   Validation loss: 0.4388 Validation accuracy: 89.32959747314453%\n",
      "Epoch 30/35 training loss: 0.1626 training accuracy: 95.1562   Validation loss: 0.5169 Validation accuracy: 87.5267105102539%\n",
      "Epoch 30/35 training loss: 0.1982 training accuracy: 93.9844   Validation loss: 0.3977 Validation accuracy: 89.93055725097656%\n",
      "Epoch 30/35 training loss: 0.1497 training accuracy: 95.7031   Validation loss: 0.3702 Validation accuracy: 90.5315170288086%\n",
      "Epoch 30/35 training loss: 0.1675 training accuracy: 94.6094   Validation loss: 0.4172 Validation accuracy: 89.93055725097656%\n",
      "Epoch 31/35 training loss: 0.1980 training accuracy: 93.0208   Validation loss: 0.4469 Validation accuracy: 89.69017028808594%\n",
      "Epoch 31/35 training loss: 0.1670 training accuracy: 95.1562   Validation loss: 0.3985 Validation accuracy: 90.05075073242188%\n",
      "Epoch 31/35 training loss: 0.1927 training accuracy: 94.4531   Validation loss: 0.3832 Validation accuracy: 90.5315170288086%\n",
      "Epoch 31/35 training loss: 0.1751 training accuracy: 94.2969   Validation loss: 0.4378 Validation accuracy: 89.93055725097656%\n",
      "Epoch 31/35 training loss: 0.1846 training accuracy: 93.7500   Validation loss: 0.4261 Validation accuracy: 89.69017028808594%\n",
      "Epoch 32/35 training loss: 0.1594 training accuracy: 94.9219   Validation loss: 0.3837 Validation accuracy: 90.5315170288086%\n",
      "Epoch 32/35 training loss: 0.1555 training accuracy: 94.9219   Validation loss: 0.4387 Validation accuracy: 90.05075073242188%\n",
      "Epoch 32/35 training loss: 0.1657 training accuracy: 95.0000   Validation loss: 0.4246 Validation accuracy: 90.29113006591797%\n",
      "Epoch 32/35 training loss: 0.1579 training accuracy: 94.7656   Validation loss: 0.4094 Validation accuracy: 90.41132354736328%\n",
      "Epoch 32/35 training loss: 0.1748 training accuracy: 94.1406   Validation loss: 0.4111 Validation accuracy: 89.20940399169922%\n",
      "Epoch 32/35 training loss: 0.2023 training accuracy: 92.9688   Validation loss: 0.4159 Validation accuracy: 89.32959747314453%\n",
      "Epoch 33/35 training loss: 0.1494 training accuracy: 95.0000   Validation loss: 0.4562 Validation accuracy: 89.32959747314453%\n",
      "Epoch 33/35 training loss: 0.1499 training accuracy: 95.2344   Validation loss: 0.3884 Validation accuracy: 89.3563003540039%\n",
      "Epoch 33/35 training loss: 0.1745 training accuracy: 95.1562   Validation loss: 0.3998 Validation accuracy: 89.44978332519531%\n",
      "Epoch 33/35 training loss: 0.1680 training accuracy: 94.1406   Validation loss: 0.4520 Validation accuracy: 88.27457427978516%\n",
      "Epoch 33/35 training loss: 0.1723 training accuracy: 94.8438   Validation loss: 0.4163 Validation accuracy: 89.93055725097656%\n",
      "Epoch 34/35 training loss: 0.1700 training accuracy: 94.4271   Validation loss: 0.3497 Validation accuracy: 91.73344421386719%\n",
      "Epoch 34/35 training loss: 0.1661 training accuracy: 95.3125   Validation loss: 0.4913 Validation accuracy: 87.5267105102539%\n",
      "Epoch 34/35 training loss: 0.1760 training accuracy: 94.3750   Validation loss: 0.4188 Validation accuracy: 88.84882354736328%\n",
      "Epoch 34/35 training loss: 0.1499 training accuracy: 95.3906   Validation loss: 0.3839 Validation accuracy: 91.13247680664062%\n",
      "Epoch 34/35 training loss: 0.1384 training accuracy: 95.1562   Validation loss: 0.3897 Validation accuracy: 90.77190399169922%\n",
      "Epoch 35/35 training loss: 0.1405 training accuracy: 96.0938   Validation loss: 0.4165 Validation accuracy: 89.20940399169922%\n",
      "Epoch 35/35 training loss: 0.1624 training accuracy: 95.1562   Validation loss: 0.4188 Validation accuracy: 89.56997680664062%\n",
      "Epoch 35/35 training loss: 0.1645 training accuracy: 95.1562   Validation loss: 0.4378 Validation accuracy: 89.44978332519531%\n",
      "Epoch 35/35 training loss: 0.1502 training accuracy: 94.7656   Validation loss: 0.4516 Validation accuracy: 88.87554168701172%\n",
      "Epoch 35/35 training loss: 0.1743 training accuracy: 94.6875   Validation loss: 0.3886 Validation accuracy: 90.17094421386719%\n"
     ]
    }
   ],
   "source": [
    "train(epochs=35, optimizer=optimizer, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your network\n",
    "\n",
    "It's good practice to test your trained network on test data, images the network has never seen either in training or validation. This will give you a good estimate for the model's performance on completely new images. Run the test images through the network and measure the accuracy, the same way you did validation. You should be able to reach around 70% accuracy on the test set if the model has been trained well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss:0.4451 Testing accuracy: 87.93016052246094%\n"
     ]
    }
   ],
   "source": [
    "#here we get the loss and accuracy on the test set to see if the model overfits, which it mostly doesn't\n",
    "test_loss, test_acc = validation(testloader, 'cuda')\n",
    "\n",
    "print('Testing loss:{:.4f}'.format(test_loss), 'Testing accuracy: {}%'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the checkpoint\n",
    "\n",
    "Now that your network is trained, save the model so you can load it later for making predictions. You probably want to save other things such as the mapping of classes to indices which you get from one of the image datasets: `image_datasets['train'].class_to_idx`. You can attach this to the model as an attribute which makes inference easier later on.\n",
    "\n",
    "```model.class_to_idx = image_datasets['train'].class_to_idx```\n",
    "\n",
    "Remember that you'll want to completely rebuild the model later so you can use it for inference. Make sure to include any information you need in the checkpoint. If you want to load the model and keep training, you'll want to save the number of epochs as well as the optimizer state, `optimizer.state_dict`. You'll likely want to use this trained model in the next part of the project, so best to save it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 0,\n",
       " '10': 1,\n",
       " '100': 2,\n",
       " '101': 3,\n",
       " '102': 4,\n",
       " '11': 5,\n",
       " '12': 6,\n",
       " '13': 7,\n",
       " '14': 8,\n",
       " '15': 9,\n",
       " '16': 10,\n",
       " '17': 11,\n",
       " '18': 12,\n",
       " '19': 13,\n",
       " '2': 14,\n",
       " '20': 15,\n",
       " '21': 16,\n",
       " '22': 17,\n",
       " '23': 18,\n",
       " '24': 19,\n",
       " '25': 20,\n",
       " '26': 21,\n",
       " '27': 22,\n",
       " '28': 23,\n",
       " '29': 24,\n",
       " '3': 25,\n",
       " '30': 26,\n",
       " '31': 27,\n",
       " '32': 28,\n",
       " '33': 29,\n",
       " '34': 30,\n",
       " '35': 31,\n",
       " '36': 32,\n",
       " '37': 33,\n",
       " '38': 34,\n",
       " '39': 35,\n",
       " '4': 36,\n",
       " '40': 37,\n",
       " '41': 38,\n",
       " '42': 39,\n",
       " '43': 40,\n",
       " '44': 41,\n",
       " '45': 42,\n",
       " '46': 43,\n",
       " '47': 44,\n",
       " '48': 45,\n",
       " '49': 46,\n",
       " '5': 47,\n",
       " '50': 48,\n",
       " '51': 49,\n",
       " '52': 50,\n",
       " '53': 51,\n",
       " '54': 52,\n",
       " '55': 53,\n",
       " '56': 54,\n",
       " '57': 55,\n",
       " '58': 56,\n",
       " '59': 57,\n",
       " '6': 58,\n",
       " '60': 59,\n",
       " '61': 60,\n",
       " '62': 61,\n",
       " '63': 62,\n",
       " '64': 63,\n",
       " '65': 64,\n",
       " '66': 65,\n",
       " '67': 66,\n",
       " '68': 67,\n",
       " '69': 68,\n",
       " '7': 69,\n",
       " '70': 70,\n",
       " '71': 71,\n",
       " '72': 72,\n",
       " '73': 73,\n",
       " '74': 74,\n",
       " '75': 75,\n",
       " '76': 76,\n",
       " '77': 77,\n",
       " '78': 78,\n",
       " '79': 79,\n",
       " '8': 80,\n",
       " '80': 81,\n",
       " '81': 82,\n",
       " '82': 83,\n",
       " '83': 84,\n",
       " '84': 85,\n",
       " '85': 86,\n",
       " '86': 87,\n",
       " '87': 88,\n",
       " '88': 89,\n",
       " '89': 90,\n",
       " '9': 91,\n",
       " '90': 92,\n",
       " '91': 93,\n",
       " '92': 94,\n",
       " '93': 95,\n",
       " '94': 96,\n",
       " '95': 97,\n",
       " '96': 98,\n",
       " '97': 99,\n",
       " '98': 100,\n",
       " '99': 101}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this dictionary captures the index associated with each class value; class values are keys, indices are values\n",
    "model.class_to_idx = train_data.class_to_idx\n",
    "model.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Sequential(\n",
       "    (fc1): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (dropout): Dropout(p=0.5)\n",
       "    (fc2): Linear(in_features=512, out_features=102, bias=True)\n",
       "    (output): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE: Save the checkpoint \n",
    "#saves the number of epochs, the entire model, the optimizer, and the class_to_idx dictionary\n",
    "checkpoint = {\n",
    "              'epochs':35,\n",
    "              'model':model, \n",
    "              'optimizer_state': optimizer.state_dict(),\n",
    "               'class_to_idx': model.class_to_idx\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(checkpoint, 'checkpoint.pth') #saves the checkpoint as a pth file for inference later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the checkpoint\n",
    "\n",
    "At this point it's good to write a function that can load a checkpoint and rebuild the model. That way you can come back to this project and keep working on it without having to retrain the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'epochs': 35, 'model': ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "    (fc): Sequential(\n",
       "      (fc1): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.5)\n",
       "      (fc2): Linear(in_features=512, out_features=102, bias=True)\n",
       "      (output): LogSoftmax()\n",
       "    )\n",
       "  ), 'optimizer_state': {'state': {140255695940488: {'step': 7175,\n",
       "     'exp_avg': tensor([[ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ...,  5.6052e-45,\n",
       "               5.6052e-45, -5.6052e-45],\n",
       "             [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ...,  5.6052e-45,\n",
       "               5.6052e-45,  5.6052e-45],\n",
       "             [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ...,  5.6052e-45,\n",
       "              -5.6052e-45,  5.6052e-45],\n",
       "             ...,\n",
       "             [-4.5133e-04, -1.6260e-04, -4.2120e-04,  ..., -1.2388e-04,\n",
       "              -3.0871e-04, -2.4138e-04],\n",
       "             [ 1.1932e-04,  3.2080e-04,  1.0878e-04,  ...,  5.4696e-04,\n",
       "               5.8084e-04,  2.0533e-04],\n",
       "             [ 5.6052e-45,  5.6052e-45, -5.6052e-45,  ...,  5.6052e-45,\n",
       "               5.6052e-45, -5.6052e-45]], device='cuda:0'),\n",
       "     'exp_avg_sq': tensor([[9.4515e-12, 1.6828e-11, 1.2538e-11,  ..., 2.2660e-11, 3.5835e-11,\n",
       "              1.5659e-12],\n",
       "             [1.2247e-10, 2.7768e-10, 3.0967e-10,  ..., 1.2924e-10, 1.0671e-10,\n",
       "              1.2139e-10],\n",
       "             [2.0163e-11, 9.0124e-11, 7.0236e-11,  ..., 4.4161e-11, 2.8751e-11,\n",
       "              2.5793e-11],\n",
       "             ...,\n",
       "             [3.0908e-06, 6.5462e-06, 5.9521e-06,  ..., 3.0576e-06, 5.3052e-06,\n",
       "              2.5060e-06],\n",
       "             [3.8604e-06, 6.2545e-06, 6.9884e-06,  ..., 3.0541e-06, 4.8785e-06,\n",
       "              2.7305e-06],\n",
       "             [1.6729e-11, 6.3180e-11, 3.5391e-11,  ..., 3.7129e-11, 1.6630e-11,\n",
       "              1.8437e-11]], device='cuda:0')},\n",
       "    140255695940128: {'step': 7175,\n",
       "     'exp_avg': tensor([ 5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45, -4.0248e-04,  5.6052e-45,  5.6052e-45, -1.5073e-04,\n",
       "              5.6052e-45, -4.9527e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             -3.1919e-04,  5.6052e-45, -5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  6.7963e-04,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -7.2936e-04,\n",
       "              1.9874e-03, -5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45, -7.5284e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45, -1.8090e-03,  5.6052e-45,  5.7617e-04,\n",
       "             -4.6560e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45, -8.8932e-04,\n",
       "              5.6052e-45,  6.4751e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  3.4116e-04,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45, -4.6176e-04,  5.6052e-45,\n",
       "             -7.0475e-04,  9.8765e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  4.2855e-05,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45, -1.6545e-03,  5.6052e-45,  5.6052e-45, -1.2934e-36,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  3.6899e-04,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -3.7345e-04,\n",
       "             -5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.1149e-04,\n",
       "              5.6052e-45, -9.6339e-06,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  9.3838e-04, -2.4924e-04,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  1.5710e-04,  5.6052e-45, -3.3448e-04,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             -1.2574e-04,  5.6052e-45,  5.6052e-45, -6.3187e-04, -6.4257e-04,\n",
       "              1.4194e-03,  5.6052e-45,  5.6052e-45,  2.9689e-04,  5.6052e-45,\n",
       "              5.6052e-45, -5.7423e-04,  2.0265e-03,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -8.4699e-04,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  8.1577e-04,  6.0539e-04,\n",
       "              5.6052e-45,  3.1994e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45, -9.2553e-04,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -6.7090e-04,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  1.4852e-03,  5.6052e-45,  5.6052e-45,\n",
       "              1.8982e-03,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              1.3208e-03,  5.6052e-45, -5.5923e-04,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             -6.2814e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -4.2904e-04,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45, -6.3399e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  1.3925e-03,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -1.8909e-03,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              1.5328e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  7.1474e-04,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45, -3.5538e-04,  6.5024e-04,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -4.0372e-04,\n",
       "             -2.1325e-04,  5.6052e-45,  5.6052e-45,  3.3285e-04, -3.1907e-04,\n",
       "             -2.2402e-03, -1.0175e-03,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45, -8.1419e-04,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45, -6.8976e-04,  5.6052e-45,  4.8272e-04,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45, -7.3350e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             -1.4000e-03,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             -6.0501e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  4.7230e-04, -5.6287e-04,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  1.8179e-04,  5.6052e-45,  5.6052e-45,  1.0764e-03,\n",
       "              1.5336e-03,  5.6052e-45,  5.6052e-45,  5.6052e-45,  1.6200e-04,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  1.7423e-03,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  6.7326e-04,\n",
       "             -1.3859e-04,  5.6052e-45,  5.6052e-45, -5.8930e-04,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45, -5.1294e-04,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45, -1.5838e-03,  5.6052e-45, -9.2968e-05,  5.6052e-45,\n",
       "              2.0756e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45, -1.9294e-03,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45, -8.9571e-05,  5.6052e-45, -5.0801e-04,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -1.6730e-03,\n",
       "              2.3717e-03,  5.6052e-45, -5.3622e-05,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45, -3.9563e-05,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              2.9661e-04,  2.5447e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -8.7498e-04,\n",
       "              8.7806e-04,  5.6052e-45], device='cuda:0'),\n",
       "     'exp_avg_sq': tensor([4.6988e-11, 8.6559e-10, 1.1006e-10, 2.3126e-11, 1.7736e-11, 9.1850e-11,\n",
       "             1.4890e-10, 1.0425e-11, 3.7977e-10, 2.0046e-10, 4.3161e-14, 1.5244e-05,\n",
       "             1.7067e-10, 3.9904e-11, 2.3196e-05, 4.1380e-11, 1.3097e-05, 1.8620e-12,\n",
       "             5.0149e-11, 6.0936e-12, 1.5819e-05, 2.1214e-10, 7.7332e-12, 1.6366e-10,\n",
       "             2.5210e-12, 7.1873e-11, 1.2750e-10, 1.1239e-10, 1.0308e-11, 1.3443e-05,\n",
       "             3.3831e-10, 1.4274e-12, 6.4892e-11, 6.5445e-11, 2.0558e-05, 1.9061e-05,\n",
       "             1.3577e-10, 4.6609e-10, 7.7662e-10, 1.5916e-10, 3.4767e-10, 2.3970e-10,\n",
       "             1.8008e-10, 5.4771e-11, 6.1607e-10, 4.5547e-10, 1.5238e-10, 3.5466e-12,\n",
       "             6.9657e-14, 1.8223e-10, 3.5637e-10, 1.6279e-05, 1.8695e-12, 2.0485e-12,\n",
       "             5.2545e-10, 2.2821e-12, 1.5333e-10, 4.6095e-10, 3.9388e-10, 1.6709e-10,\n",
       "             1.6470e-11, 2.8129e-10, 1.9164e-05, 1.2482e-10, 1.3711e-05, 1.2718e-05,\n",
       "             4.5364e-10, 2.0261e-10, 1.5476e-10, 1.5171e-05, 1.0285e-10, 1.7001e-05,\n",
       "             3.4022e-12, 1.1431e-11, 4.4643e-12, 4.3220e-10, 1.9194e-10, 1.1288e-12,\n",
       "             2.0477e-05, 2.1838e-10, 5.5024e-10, 2.8844e-12, 4.8890e-10, 2.2240e-05,\n",
       "             3.1589e-10, 1.6656e-05, 1.3168e-05, 1.7635e-13, 8.0704e-13, 1.5625e-11,\n",
       "             1.1956e-11, 1.1516e-12, 1.0768e-12, 6.2483e-10, 1.3200e-05, 1.0096e-10,\n",
       "             1.5230e-09, 9.9209e-11, 3.6090e-10, 3.2030e-11, 1.8068e-12, 2.4375e-05,\n",
       "             4.2047e-11, 1.2043e-12, 1.0654e-13, 4.4341e-12, 9.4519e-10, 2.6371e-13,\n",
       "             2.1390e-11, 1.4983e-05, 1.4045e-11, 1.6346e-11, 6.9178e-11, 3.2210e-10,\n",
       "             1.6310e-05, 2.7274e-10, 3.4347e-10, 2.6285e-11, 4.5943e-14, 1.5270e-05,\n",
       "             1.0052e-10, 1.5660e-05, 1.8859e-11, 8.9127e-11, 1.7019e-11, 8.9069e-14,\n",
       "             7.9670e-10, 6.5973e-12, 4.6085e-10, 1.8993e-10, 1.3559e-09, 3.3859e-10,\n",
       "             1.1969e-05, 1.5969e-05, 2.7086e-11, 2.4520e-13, 2.3052e-12, 8.6867e-11,\n",
       "             3.9053e-10, 2.7091e-11, 4.9476e-12, 5.5110e-12, 1.8839e-05, 4.5511e-11,\n",
       "             1.5725e-05, 5.3253e-11, 1.4137e-10, 1.3047e-11, 5.0569e-12, 2.0423e-10,\n",
       "             1.3543e-05, 2.0355e-10, 1.1228e-10, 1.3202e-05, 1.8195e-05, 2.4320e-05,\n",
       "             1.3061e-10, 3.0206e-11, 1.4092e-05, 8.1774e-12, 6.0108e-11, 2.0384e-05,\n",
       "             1.6802e-05, 2.7914e-11, 2.5347e-12, 1.3106e-09, 7.7453e-11, 2.2406e-11,\n",
       "             1.1651e-10, 1.4938e-05, 2.2926e-12, 6.3788e-11, 1.3338e-10, 3.6574e-15,\n",
       "             2.1232e-10, 5.5452e-11, 9.8383e-11, 4.4897e-11, 1.9193e-05, 1.3445e-05,\n",
       "             1.4892e-10, 1.9196e-05, 4.7924e-10, 1.3239e-10, 2.6671e-10, 6.4217e-12,\n",
       "             3.5555e-11, 3.2896e-10, 6.3331e-13, 1.0881e-12, 1.4933e-14, 1.8048e-12,\n",
       "             4.1861e-11, 1.7777e-05, 2.7520e-10, 2.3904e-10, 1.0204e-11, 6.6750e-11,\n",
       "             5.0858e-12, 2.1022e-05, 5.7224e-10, 9.0697e-10, 1.4027e-11, 5.6010e-12,\n",
       "             4.0021e-12, 1.9389e-12, 1.0187e-10, 3.6136e-10, 5.5055e-10, 1.4602e-12,\n",
       "             6.5104e-10, 1.0534e-09, 1.7360e-05, 5.8070e-11, 3.7558e-10, 1.8651e-05,\n",
       "             1.6084e-11, 5.7428e-12, 1.2494e-11, 3.3490e-11, 1.6153e-05, 1.0590e-10,\n",
       "             2.3553e-05, 2.5756e-10, 5.7817e-10, 1.0853e-09, 3.6906e-11, 3.8893e-12,\n",
       "             2.5671e-14, 8.6350e-12, 1.5423e-05, 2.1917e-10, 6.1832e-12, 2.0260e-10,\n",
       "             4.1394e-10, 7.7695e-11, 7.3682e-11, 2.9502e-12, 1.1122e-10, 1.2966e-05,\n",
       "             5.9839e-10, 3.6783e-11, 1.9912e-11, 2.9175e-10, 1.9623e-11, 5.2357e-11,\n",
       "             1.2411e-05, 4.8216e-10, 2.8805e-11, 3.5729e-11, 2.5519e-10, 2.4207e-12,\n",
       "             3.1704e-11, 1.6762e-05, 2.0820e-10, 6.2537e-10, 9.7847e-11, 1.3147e-11,\n",
       "             1.1935e-10, 1.5288e-05, 5.4730e-10, 2.7611e-11, 6.4035e-12, 4.9467e-11,\n",
       "             4.2586e-11, 1.5205e-05, 1.6590e-10, 9.9644e-12, 6.3213e-15, 1.2975e-11,\n",
       "             4.4302e-10, 1.6683e-12, 1.5621e-10, 5.4408e-10, 6.0658e-11, 9.7557e-11,\n",
       "             4.0102e-11, 2.0783e-11, 5.1218e-10, 8.5161e-11, 6.9912e-11, 1.2302e-10,\n",
       "             1.5615e-05, 6.3030e-11, 4.8066e-12, 1.5820e-11, 1.1776e-10, 1.0680e-11,\n",
       "             2.4712e-10, 2.8924e-12, 7.3666e-10, 1.3970e-05, 1.3369e-05, 3.8323e-10,\n",
       "             8.1448e-14, 4.8187e-12, 5.1799e-14, 3.3576e-10, 2.5556e-12, 2.9205e-12,\n",
       "             5.4112e-12, 6.1569e-10, 7.0052e-10, 3.4250e-12, 5.8084e-11, 1.0191e-10,\n",
       "             1.5400e-10, 3.0746e-11, 2.4528e-10, 1.6032e-05, 1.7859e-05, 1.0330e-11,\n",
       "             1.2489e-11, 1.6154e-05, 1.4379e-05, 1.6623e-05, 1.5489e-05, 4.5282e-12,\n",
       "             3.3949e-10, 1.7134e-13, 5.0376e-11, 2.5908e-10, 2.9866e-12, 1.9194e-05,\n",
       "             6.7789e-11, 1.8549e-10, 3.5628e-10, 1.5880e-05, 3.4546e-10, 1.8766e-05,\n",
       "             1.1746e-09, 3.5068e-10, 1.0384e-10, 4.4107e-12, 4.5942e-12, 7.5716e-12,\n",
       "             6.0744e-10, 2.7327e-11, 7.3547e-10, 1.9764e-11, 1.5361e-11, 1.8482e-05,\n",
       "             1.3107e-11, 2.5439e-10, 5.6223e-12, 2.0813e-05, 4.1898e-10, 4.5236e-11,\n",
       "             9.5030e-13, 3.9323e-10, 2.4988e-10, 1.0735e-10, 7.4714e-12, 9.3461e-11,\n",
       "             2.8268e-11, 1.7278e-10, 1.3735e-10, 1.0765e-09, 1.1204e-10, 2.4664e-10,\n",
       "             2.8347e-10, 2.3988e-12, 4.5463e-12, 2.3042e-11, 2.5024e-11, 1.6855e-05,\n",
       "             7.5782e-11, 2.4515e-12, 4.7176e-12, 1.7671e-11, 5.4524e-10, 1.1895e-05,\n",
       "             1.5032e-05, 9.1356e-11, 1.3569e-10, 1.3772e-11, 4.2367e-10, 2.1194e-12,\n",
       "             1.8421e-10, 1.0208e-10, 1.8028e-11, 3.7238e-11, 3.0497e-11, 4.5766e-12,\n",
       "             6.2266e-10, 2.8605e-11, 1.1159e-11, 4.5940e-10, 6.3109e-10, 1.3863e-11,\n",
       "             5.2521e-11, 2.7923e-10, 1.7079e-11, 2.2155e-11, 4.2907e-10, 1.5363e-10,\n",
       "             1.5002e-05, 7.6290e-12, 8.2428e-11, 1.5441e-05, 1.9006e-05, 3.7239e-12,\n",
       "             1.9242e-12, 1.0556e-10, 1.8297e-05, 2.5755e-11, 1.0638e-12, 2.5324e-10,\n",
       "             1.1053e-10, 1.2868e-10, 7.3179e-10, 3.8335e-10, 9.0547e-11, 1.8134e-10,\n",
       "             7.6622e-13, 1.4471e-13, 1.4567e-05, 8.8050e-10, 3.5395e-11, 1.1248e-11,\n",
       "             4.9278e-11, 2.5516e-11, 2.2492e-11, 2.6575e-11, 1.3510e-05, 1.6700e-05,\n",
       "             7.5800e-10, 1.8917e-11, 1.9226e-05, 6.2333e-10, 2.6045e-11, 1.2399e-10,\n",
       "             1.6532e-05, 2.1955e-10, 2.9716e-10, 1.0393e-10, 2.2903e-11, 2.1218e-11,\n",
       "             4.7150e-11, 2.3718e-11, 1.0063e-09, 2.7060e-10, 1.2180e-10, 2.4464e-12,\n",
       "             9.5469e-11, 1.7037e-10, 1.9608e-05, 2.2556e-11, 2.2710e-05, 1.0872e-10,\n",
       "             1.5798e-05, 2.8204e-11, 1.2158e-09, 5.2324e-11, 1.6828e-05, 1.9929e-11,\n",
       "             2.1513e-10, 6.0242e-10, 1.9662e-10, 7.2367e-12, 1.6472e-11, 4.0503e-11,\n",
       "             1.7981e-05, 4.3888e-11, 2.1622e-05, 5.7841e-11, 2.3479e-10, 5.1334e-12,\n",
       "             8.2822e-14, 6.0424e-12, 7.0232e-11, 6.0652e-12, 3.9158e-10, 3.3730e-12,\n",
       "             1.6748e-10, 1.3865e-10, 4.7947e-10, 4.0262e-12, 2.6132e-11, 1.5675e-05,\n",
       "             1.6789e-05, 2.9524e-10, 2.0549e-05, 3.3681e-10, 1.1670e-11, 4.1150e-13,\n",
       "             5.8569e-12, 1.4544e-12, 1.3769e-10, 8.3790e-10, 4.2125e-10, 2.2313e-14,\n",
       "             1.3798e-05, 9.9761e-11, 2.2585e-12, 1.3900e-13, 4.9884e-12, 3.1645e-12,\n",
       "             6.1021e-11, 4.5777e-11, 1.5683e-05, 1.6281e-05, 5.5707e-10, 3.4140e-10,\n",
       "             1.2145e-10, 4.2486e-11, 2.9711e-10, 4.5795e-12, 2.0076e-10, 1.6122e-05,\n",
       "             1.4501e-05, 1.4051e-10], device='cuda:0')},\n",
       "    140257463675568: {'step': 7175,\n",
       "     'exp_avg': tensor([[ 5.6052e-45, -5.6052e-45,  5.6052e-45,  ...,  4.1852e-04,\n",
       "               4.0991e-03,  5.6052e-45],\n",
       "             [ 5.6052e-45, -5.6052e-45,  5.6052e-45,  ...,  1.7118e-07,\n",
       "               4.1645e-04, -5.6052e-45],\n",
       "             [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ...,  6.5819e-06,\n",
       "               2.3782e-04,  5.6052e-45],\n",
       "             ...,\n",
       "             [ 5.6052e-45, -5.6052e-45,  5.6052e-45,  ..., -5.3580e-03,\n",
       "              -2.2141e-02,  5.6052e-45],\n",
       "             [ 5.6052e-45, -5.6052e-45,  5.6052e-45,  ..., -1.3751e-03,\n",
       "              -1.4262e-03, -5.6052e-45],\n",
       "             [-5.6052e-45,  5.6052e-45,  5.6052e-45,  ...,  9.8031e-04,\n",
       "              -2.5853e-04,  5.6052e-45]], device='cuda:0'),\n",
       "     'exp_avg_sq': tensor([[6.0084e-12, 1.7851e-08, 2.2400e-11,  ..., 5.8353e-04, 1.6189e-03,\n",
       "              3.8659e-11],\n",
       "             [3.1611e-12, 4.9662e-08, 1.4375e-11,  ..., 1.0414e-05, 1.1698e-05,\n",
       "              6.7128e-11],\n",
       "             [1.9858e-11, 2.8886e-08, 8.8118e-11,  ..., 1.5863e-05, 4.8161e-04,\n",
       "              6.5026e-11],\n",
       "             ...,\n",
       "             [5.0194e-11, 2.7748e-08, 1.0316e-10,  ..., 1.8851e-03, 5.6230e-03,\n",
       "              7.9006e-11],\n",
       "             [1.1896e-12, 3.6737e-08, 3.9236e-11,  ..., 1.9660e-03, 1.2282e-03,\n",
       "              3.3505e-10],\n",
       "             [9.4576e-10, 4.2871e-08, 3.5886e-10,  ..., 3.7927e-04, 2.2089e-04,\n",
       "              1.3641e-10]], device='cuda:0')},\n",
       "    140257464123752: {'step': 7175,\n",
       "     'exp_avg': tensor([ 3.7828e-04,  2.3444e-04,  5.1359e-05, -1.2978e-03, -1.7299e-05,\n",
       "              1.1067e-03,  5.2396e-04,  1.8536e-04, -2.8406e-05,  2.7450e-07,\n",
       "             -2.8356e-04,  2.7060e-05, -1.6045e-03,  8.9706e-04,  2.7248e-05,\n",
       "             -4.5981e-04, -2.8434e-05,  3.3611e-04, -1.6504e-03,  4.2978e-04,\n",
       "              4.7040e-04,  1.2665e-03,  5.0173e-05, -4.1383e-04,  5.1448e-05,\n",
       "             -5.7193e-04, -3.8530e-04, -1.5588e-03,  1.7041e-04, -2.5528e-06,\n",
       "             -2.2007e-05, -2.4403e-04,  5.2140e-04,  1.0808e-03, -4.3590e-05,\n",
       "              1.3704e-03, -2.0254e-04,  3.9756e-03, -1.1376e-04,  1.1696e-04,\n",
       "              5.0006e-03, -2.5422e-03,  8.6752e-05,  2.2562e-04, -1.7725e-06,\n",
       "             -2.1541e-04,  2.7146e-06, -1.6571e-05, -4.2588e-04,  3.6931e-03,\n",
       "             -2.1982e-05,  5.8460e-04,  5.9420e-05,  3.4973e-04, -3.5999e-05,\n",
       "             -3.7185e-04, -9.0493e-05, -2.1916e-03,  1.8461e-04,  7.0462e-07,\n",
       "             -4.3489e-05, -2.8047e-04,  2.2824e-05, -4.3340e-06,  1.1785e-05,\n",
       "             -7.3233e-05,  6.6583e-04, -2.8141e-05,  2.7695e-05, -3.0455e-05,\n",
       "             -4.6824e-05,  2.1906e-03, -3.6530e-03, -2.6376e-03,  3.1250e-03,\n",
       "             -5.6074e-04,  6.6627e-04,  5.1753e-05,  4.7365e-04, -2.4472e-06,\n",
       "              3.0794e-04,  9.6281e-05,  6.1292e-05, -4.9306e-05,  9.6469e-05,\n",
       "             -3.0638e-04, -9.0065e-04, -4.3584e-04, -1.0963e-03, -1.6079e-03,\n",
       "              6.1994e-04, -3.9110e-04, -9.3154e-04,  1.8914e-03,  1.3090e-04,\n",
       "              2.3277e-04, -1.4596e-04, -8.0349e-05, -2.4135e-03, -2.9800e-03,\n",
       "             -9.3142e-04,  3.4142e-04], device='cuda:0'),\n",
       "     'exp_avg_sq': tensor([1.8878e-05, 7.9716e-06, 5.8590e-06, 3.9624e-05, 2.2750e-06, 4.7652e-05,\n",
       "             2.2245e-05, 9.7345e-06, 1.0684e-05, 1.3816e-05, 2.3145e-05, 9.5993e-06,\n",
       "             3.8338e-05, 2.3718e-05, 6.5689e-06, 2.0518e-05, 1.4103e-05, 2.0412e-05,\n",
       "             1.0563e-05, 2.7665e-05, 4.7485e-06, 1.5765e-05, 6.5040e-06, 2.0485e-05,\n",
       "             1.8554e-05, 5.8488e-05, 4.4134e-05, 3.9680e-05, 3.0738e-05, 1.4115e-05,\n",
       "             4.7744e-06, 8.8474e-06, 3.5257e-05, 1.2949e-05, 1.1873e-05, 3.9167e-05,\n",
       "             6.8591e-05, 4.6693e-05, 1.9829e-05, 2.4877e-05, 1.0743e-04, 1.5725e-05,\n",
       "             2.7825e-05, 3.3714e-05, 3.3049e-06, 1.9532e-05, 5.3002e-06, 1.6960e-05,\n",
       "             1.8699e-05, 1.0472e-04, 1.6714e-05, 3.0522e-05, 9.4499e-06, 2.8292e-05,\n",
       "             8.4814e-06, 1.5342e-05, 6.8012e-06, 6.9988e-06, 2.7856e-06, 2.3613e-06,\n",
       "             7.2009e-06, 2.3678e-05, 2.4924e-06, 2.3895e-06, 1.2854e-05, 2.4646e-06,\n",
       "             2.3201e-05, 3.3269e-05, 1.1952e-05, 1.2502e-05, 1.2060e-05, 6.2513e-06,\n",
       "             7.5464e-05, 4.5817e-05, 8.0050e-05, 2.9274e-05, 3.2670e-05, 2.4059e-05,\n",
       "             4.1161e-05, 4.4037e-06, 8.1695e-06, 2.1860e-05, 1.2069e-05, 5.9490e-05,\n",
       "             6.4735e-05, 6.0471e-05, 2.4285e-05, 1.3623e-05, 3.0924e-05, 6.7633e-05,\n",
       "             5.4147e-05, 2.3758e-05, 6.3520e-05, 2.8298e-05, 2.3318e-05, 1.8644e-05,\n",
       "             2.8240e-05, 7.6522e-05, 8.8149e-05, 7.9229e-05, 4.2329e-05, 1.0971e-05],\n",
       "            device='cuda:0')}},\n",
       "   'param_groups': [{'lr': 0.001,\n",
       "     'betas': (0.9, 0.999),\n",
       "     'eps': 1e-08,\n",
       "     'weight_decay': 0,\n",
       "     'amsgrad': False,\n",
       "     'params': [140255695940488,\n",
       "      140255695940128,\n",
       "      140257463675568,\n",
       "      140257464123752]}]}, 'class_to_idx': {'1': 0,\n",
       "   '10': 1,\n",
       "   '100': 2,\n",
       "   '101': 3,\n",
       "   '102': 4,\n",
       "   '11': 5,\n",
       "   '12': 6,\n",
       "   '13': 7,\n",
       "   '14': 8,\n",
       "   '15': 9,\n",
       "   '16': 10,\n",
       "   '17': 11,\n",
       "   '18': 12,\n",
       "   '19': 13,\n",
       "   '2': 14,\n",
       "   '20': 15,\n",
       "   '21': 16,\n",
       "   '22': 17,\n",
       "   '23': 18,\n",
       "   '24': 19,\n",
       "   '25': 20,\n",
       "   '26': 21,\n",
       "   '27': 22,\n",
       "   '28': 23,\n",
       "   '29': 24,\n",
       "   '3': 25,\n",
       "   '30': 26,\n",
       "   '31': 27,\n",
       "   '32': 28,\n",
       "   '33': 29,\n",
       "   '34': 30,\n",
       "   '35': 31,\n",
       "   '36': 32,\n",
       "   '37': 33,\n",
       "   '38': 34,\n",
       "   '39': 35,\n",
       "   '4': 36,\n",
       "   '40': 37,\n",
       "   '41': 38,\n",
       "   '42': 39,\n",
       "   '43': 40,\n",
       "   '44': 41,\n",
       "   '45': 42,\n",
       "   '46': 43,\n",
       "   '47': 44,\n",
       "   '48': 45,\n",
       "   '49': 46,\n",
       "   '5': 47,\n",
       "   '50': 48,\n",
       "   '51': 49,\n",
       "   '52': 50,\n",
       "   '53': 51,\n",
       "   '54': 52,\n",
       "   '55': 53,\n",
       "   '56': 54,\n",
       "   '57': 55,\n",
       "   '58': 56,\n",
       "   '59': 57,\n",
       "   '6': 58,\n",
       "   '60': 59,\n",
       "   '61': 60,\n",
       "   '62': 61,\n",
       "   '63': 62,\n",
       "   '64': 63,\n",
       "   '65': 64,\n",
       "   '66': 65,\n",
       "   '67': 66,\n",
       "   '68': 67,\n",
       "   '69': 68,\n",
       "   '7': 69,\n",
       "   '70': 70,\n",
       "   '71': 71,\n",
       "   '72': 72,\n",
       "   '73': 73,\n",
       "   '74': 74,\n",
       "   '75': 75,\n",
       "   '76': 76,\n",
       "   '77': 77,\n",
       "   '78': 78,\n",
       "   '79': 79,\n",
       "   '8': 80,\n",
       "   '80': 81,\n",
       "   '81': 82,\n",
       "   '82': 83,\n",
       "   '83': 84,\n",
       "   '84': 85,\n",
       "   '85': 86,\n",
       "   '86': 87,\n",
       "   '87': 88,\n",
       "   '88': 89,\n",
       "   '89': 90,\n",
       "   '9': 91,\n",
       "   '90': 92,\n",
       "   '91': 93,\n",
       "   '92': 94,\n",
       "   '93': 95,\n",
       "   '94': 96,\n",
       "   '95': 97,\n",
       "   '96': 98,\n",
       "   '97': 99,\n",
       "   '98': 100,\n",
       "   '99': 101}},\n",
       " {'state': {140255695940488: {'step': 7175,\n",
       "    'exp_avg': tensor([[ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ...,  5.6052e-45,\n",
       "              5.6052e-45, -5.6052e-45],\n",
       "            [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ...,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45],\n",
       "            [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ...,  5.6052e-45,\n",
       "             -5.6052e-45,  5.6052e-45],\n",
       "            ...,\n",
       "            [-4.5133e-04, -1.6260e-04, -4.2120e-04,  ..., -1.2388e-04,\n",
       "             -3.0871e-04, -2.4138e-04],\n",
       "            [ 1.1932e-04,  3.2080e-04,  1.0878e-04,  ...,  5.4696e-04,\n",
       "              5.8084e-04,  2.0533e-04],\n",
       "            [ 5.6052e-45,  5.6052e-45, -5.6052e-45,  ...,  5.6052e-45,\n",
       "              5.6052e-45, -5.6052e-45]], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([[9.4515e-12, 1.6828e-11, 1.2538e-11,  ..., 2.2660e-11, 3.5835e-11,\n",
       "             1.5659e-12],\n",
       "            [1.2247e-10, 2.7768e-10, 3.0967e-10,  ..., 1.2924e-10, 1.0671e-10,\n",
       "             1.2139e-10],\n",
       "            [2.0163e-11, 9.0124e-11, 7.0236e-11,  ..., 4.4161e-11, 2.8751e-11,\n",
       "             2.5793e-11],\n",
       "            ...,\n",
       "            [3.0908e-06, 6.5462e-06, 5.9521e-06,  ..., 3.0576e-06, 5.3052e-06,\n",
       "             2.5060e-06],\n",
       "            [3.8604e-06, 6.2545e-06, 6.9884e-06,  ..., 3.0541e-06, 4.8785e-06,\n",
       "             2.7305e-06],\n",
       "            [1.6729e-11, 6.3180e-11, 3.5391e-11,  ..., 3.7129e-11, 1.6630e-11,\n",
       "             1.8437e-11]], device='cuda:0')},\n",
       "   140255695940128: {'step': 7175,\n",
       "    'exp_avg': tensor([ 5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45, -4.0248e-04,  5.6052e-45,  5.6052e-45, -1.5073e-04,\n",
       "             5.6052e-45, -4.9527e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "            -3.1919e-04,  5.6052e-45, -5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  6.7963e-04,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -7.2936e-04,\n",
       "             1.9874e-03, -5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45, -7.5284e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45, -1.8090e-03,  5.6052e-45,  5.7617e-04,\n",
       "            -4.6560e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45, -8.8932e-04,\n",
       "             5.6052e-45,  6.4751e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  3.4116e-04,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45, -4.6176e-04,  5.6052e-45,\n",
       "            -7.0475e-04,  9.8765e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  4.2855e-05,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45, -1.6545e-03,  5.6052e-45,  5.6052e-45, -1.2934e-36,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  3.6899e-04,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -3.7345e-04,\n",
       "            -5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.1149e-04,\n",
       "             5.6052e-45, -9.6339e-06,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  9.3838e-04, -2.4924e-04,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  1.5710e-04,  5.6052e-45, -3.3448e-04,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "            -1.2574e-04,  5.6052e-45,  5.6052e-45, -6.3187e-04, -6.4257e-04,\n",
       "             1.4194e-03,  5.6052e-45,  5.6052e-45,  2.9689e-04,  5.6052e-45,\n",
       "             5.6052e-45, -5.7423e-04,  2.0265e-03,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -8.4699e-04,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  8.1577e-04,  6.0539e-04,\n",
       "             5.6052e-45,  3.1994e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45, -9.2553e-04,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -6.7090e-04,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  1.4852e-03,  5.6052e-45,  5.6052e-45,\n",
       "             1.8982e-03,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             1.3208e-03,  5.6052e-45, -5.5923e-04,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "            -6.2814e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -4.2904e-04,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45, -6.3399e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  1.3925e-03,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -1.8909e-03,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             1.5328e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  7.1474e-04,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45, -3.5538e-04,  6.5024e-04,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -4.0372e-04,\n",
       "            -2.1325e-04,  5.6052e-45,  5.6052e-45,  3.3285e-04, -3.1907e-04,\n",
       "            -2.2402e-03, -1.0175e-03,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45, -8.1419e-04,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45, -6.8976e-04,  5.6052e-45,  4.8272e-04,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45, -7.3350e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "            -1.4000e-03,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "            -6.0501e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  4.7230e-04, -5.6287e-04,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  1.8179e-04,  5.6052e-45,  5.6052e-45,  1.0764e-03,\n",
       "             1.5336e-03,  5.6052e-45,  5.6052e-45,  5.6052e-45,  1.6200e-04,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  1.7423e-03,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  6.7326e-04,\n",
       "            -1.3859e-04,  5.6052e-45,  5.6052e-45, -5.8930e-04,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45, -5.1294e-04,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45, -1.5838e-03,  5.6052e-45, -9.2968e-05,  5.6052e-45,\n",
       "             2.0756e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45, -1.9294e-03,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45, -8.9571e-05,  5.6052e-45, -5.0801e-04,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -1.6730e-03,\n",
       "             2.3717e-03,  5.6052e-45, -5.3622e-05,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45, -3.9563e-05,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             2.9661e-04,  2.5447e-04,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -8.7498e-04,\n",
       "             8.7806e-04,  5.6052e-45], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([4.6988e-11, 8.6559e-10, 1.1006e-10, 2.3126e-11, 1.7736e-11, 9.1850e-11,\n",
       "            1.4890e-10, 1.0425e-11, 3.7977e-10, 2.0046e-10, 4.3161e-14, 1.5244e-05,\n",
       "            1.7067e-10, 3.9904e-11, 2.3196e-05, 4.1380e-11, 1.3097e-05, 1.8620e-12,\n",
       "            5.0149e-11, 6.0936e-12, 1.5819e-05, 2.1214e-10, 7.7332e-12, 1.6366e-10,\n",
       "            2.5210e-12, 7.1873e-11, 1.2750e-10, 1.1239e-10, 1.0308e-11, 1.3443e-05,\n",
       "            3.3831e-10, 1.4274e-12, 6.4892e-11, 6.5445e-11, 2.0558e-05, 1.9061e-05,\n",
       "            1.3577e-10, 4.6609e-10, 7.7662e-10, 1.5916e-10, 3.4767e-10, 2.3970e-10,\n",
       "            1.8008e-10, 5.4771e-11, 6.1607e-10, 4.5547e-10, 1.5238e-10, 3.5466e-12,\n",
       "            6.9657e-14, 1.8223e-10, 3.5637e-10, 1.6279e-05, 1.8695e-12, 2.0485e-12,\n",
       "            5.2545e-10, 2.2821e-12, 1.5333e-10, 4.6095e-10, 3.9388e-10, 1.6709e-10,\n",
       "            1.6470e-11, 2.8129e-10, 1.9164e-05, 1.2482e-10, 1.3711e-05, 1.2718e-05,\n",
       "            4.5364e-10, 2.0261e-10, 1.5476e-10, 1.5171e-05, 1.0285e-10, 1.7001e-05,\n",
       "            3.4022e-12, 1.1431e-11, 4.4643e-12, 4.3220e-10, 1.9194e-10, 1.1288e-12,\n",
       "            2.0477e-05, 2.1838e-10, 5.5024e-10, 2.8844e-12, 4.8890e-10, 2.2240e-05,\n",
       "            3.1589e-10, 1.6656e-05, 1.3168e-05, 1.7635e-13, 8.0704e-13, 1.5625e-11,\n",
       "            1.1956e-11, 1.1516e-12, 1.0768e-12, 6.2483e-10, 1.3200e-05, 1.0096e-10,\n",
       "            1.5230e-09, 9.9209e-11, 3.6090e-10, 3.2030e-11, 1.8068e-12, 2.4375e-05,\n",
       "            4.2047e-11, 1.2043e-12, 1.0654e-13, 4.4341e-12, 9.4519e-10, 2.6371e-13,\n",
       "            2.1390e-11, 1.4983e-05, 1.4045e-11, 1.6346e-11, 6.9178e-11, 3.2210e-10,\n",
       "            1.6310e-05, 2.7274e-10, 3.4347e-10, 2.6285e-11, 4.5943e-14, 1.5270e-05,\n",
       "            1.0052e-10, 1.5660e-05, 1.8859e-11, 8.9127e-11, 1.7019e-11, 8.9069e-14,\n",
       "            7.9670e-10, 6.5973e-12, 4.6085e-10, 1.8993e-10, 1.3559e-09, 3.3859e-10,\n",
       "            1.1969e-05, 1.5969e-05, 2.7086e-11, 2.4520e-13, 2.3052e-12, 8.6867e-11,\n",
       "            3.9053e-10, 2.7091e-11, 4.9476e-12, 5.5110e-12, 1.8839e-05, 4.5511e-11,\n",
       "            1.5725e-05, 5.3253e-11, 1.4137e-10, 1.3047e-11, 5.0569e-12, 2.0423e-10,\n",
       "            1.3543e-05, 2.0355e-10, 1.1228e-10, 1.3202e-05, 1.8195e-05, 2.4320e-05,\n",
       "            1.3061e-10, 3.0206e-11, 1.4092e-05, 8.1774e-12, 6.0108e-11, 2.0384e-05,\n",
       "            1.6802e-05, 2.7914e-11, 2.5347e-12, 1.3106e-09, 7.7453e-11, 2.2406e-11,\n",
       "            1.1651e-10, 1.4938e-05, 2.2926e-12, 6.3788e-11, 1.3338e-10, 3.6574e-15,\n",
       "            2.1232e-10, 5.5452e-11, 9.8383e-11, 4.4897e-11, 1.9193e-05, 1.3445e-05,\n",
       "            1.4892e-10, 1.9196e-05, 4.7924e-10, 1.3239e-10, 2.6671e-10, 6.4217e-12,\n",
       "            3.5555e-11, 3.2896e-10, 6.3331e-13, 1.0881e-12, 1.4933e-14, 1.8048e-12,\n",
       "            4.1861e-11, 1.7777e-05, 2.7520e-10, 2.3904e-10, 1.0204e-11, 6.6750e-11,\n",
       "            5.0858e-12, 2.1022e-05, 5.7224e-10, 9.0697e-10, 1.4027e-11, 5.6010e-12,\n",
       "            4.0021e-12, 1.9389e-12, 1.0187e-10, 3.6136e-10, 5.5055e-10, 1.4602e-12,\n",
       "            6.5104e-10, 1.0534e-09, 1.7360e-05, 5.8070e-11, 3.7558e-10, 1.8651e-05,\n",
       "            1.6084e-11, 5.7428e-12, 1.2494e-11, 3.3490e-11, 1.6153e-05, 1.0590e-10,\n",
       "            2.3553e-05, 2.5756e-10, 5.7817e-10, 1.0853e-09, 3.6906e-11, 3.8893e-12,\n",
       "            2.5671e-14, 8.6350e-12, 1.5423e-05, 2.1917e-10, 6.1832e-12, 2.0260e-10,\n",
       "            4.1394e-10, 7.7695e-11, 7.3682e-11, 2.9502e-12, 1.1122e-10, 1.2966e-05,\n",
       "            5.9839e-10, 3.6783e-11, 1.9912e-11, 2.9175e-10, 1.9623e-11, 5.2357e-11,\n",
       "            1.2411e-05, 4.8216e-10, 2.8805e-11, 3.5729e-11, 2.5519e-10, 2.4207e-12,\n",
       "            3.1704e-11, 1.6762e-05, 2.0820e-10, 6.2537e-10, 9.7847e-11, 1.3147e-11,\n",
       "            1.1935e-10, 1.5288e-05, 5.4730e-10, 2.7611e-11, 6.4035e-12, 4.9467e-11,\n",
       "            4.2586e-11, 1.5205e-05, 1.6590e-10, 9.9644e-12, 6.3213e-15, 1.2975e-11,\n",
       "            4.4302e-10, 1.6683e-12, 1.5621e-10, 5.4408e-10, 6.0658e-11, 9.7557e-11,\n",
       "            4.0102e-11, 2.0783e-11, 5.1218e-10, 8.5161e-11, 6.9912e-11, 1.2302e-10,\n",
       "            1.5615e-05, 6.3030e-11, 4.8066e-12, 1.5820e-11, 1.1776e-10, 1.0680e-11,\n",
       "            2.4712e-10, 2.8924e-12, 7.3666e-10, 1.3970e-05, 1.3369e-05, 3.8323e-10,\n",
       "            8.1448e-14, 4.8187e-12, 5.1799e-14, 3.3576e-10, 2.5556e-12, 2.9205e-12,\n",
       "            5.4112e-12, 6.1569e-10, 7.0052e-10, 3.4250e-12, 5.8084e-11, 1.0191e-10,\n",
       "            1.5400e-10, 3.0746e-11, 2.4528e-10, 1.6032e-05, 1.7859e-05, 1.0330e-11,\n",
       "            1.2489e-11, 1.6154e-05, 1.4379e-05, 1.6623e-05, 1.5489e-05, 4.5282e-12,\n",
       "            3.3949e-10, 1.7134e-13, 5.0376e-11, 2.5908e-10, 2.9866e-12, 1.9194e-05,\n",
       "            6.7789e-11, 1.8549e-10, 3.5628e-10, 1.5880e-05, 3.4546e-10, 1.8766e-05,\n",
       "            1.1746e-09, 3.5068e-10, 1.0384e-10, 4.4107e-12, 4.5942e-12, 7.5716e-12,\n",
       "            6.0744e-10, 2.7327e-11, 7.3547e-10, 1.9764e-11, 1.5361e-11, 1.8482e-05,\n",
       "            1.3107e-11, 2.5439e-10, 5.6223e-12, 2.0813e-05, 4.1898e-10, 4.5236e-11,\n",
       "            9.5030e-13, 3.9323e-10, 2.4988e-10, 1.0735e-10, 7.4714e-12, 9.3461e-11,\n",
       "            2.8268e-11, 1.7278e-10, 1.3735e-10, 1.0765e-09, 1.1204e-10, 2.4664e-10,\n",
       "            2.8347e-10, 2.3988e-12, 4.5463e-12, 2.3042e-11, 2.5024e-11, 1.6855e-05,\n",
       "            7.5782e-11, 2.4515e-12, 4.7176e-12, 1.7671e-11, 5.4524e-10, 1.1895e-05,\n",
       "            1.5032e-05, 9.1356e-11, 1.3569e-10, 1.3772e-11, 4.2367e-10, 2.1194e-12,\n",
       "            1.8421e-10, 1.0208e-10, 1.8028e-11, 3.7238e-11, 3.0497e-11, 4.5766e-12,\n",
       "            6.2266e-10, 2.8605e-11, 1.1159e-11, 4.5940e-10, 6.3109e-10, 1.3863e-11,\n",
       "            5.2521e-11, 2.7923e-10, 1.7079e-11, 2.2155e-11, 4.2907e-10, 1.5363e-10,\n",
       "            1.5002e-05, 7.6290e-12, 8.2428e-11, 1.5441e-05, 1.9006e-05, 3.7239e-12,\n",
       "            1.9242e-12, 1.0556e-10, 1.8297e-05, 2.5755e-11, 1.0638e-12, 2.5324e-10,\n",
       "            1.1053e-10, 1.2868e-10, 7.3179e-10, 3.8335e-10, 9.0547e-11, 1.8134e-10,\n",
       "            7.6622e-13, 1.4471e-13, 1.4567e-05, 8.8050e-10, 3.5395e-11, 1.1248e-11,\n",
       "            4.9278e-11, 2.5516e-11, 2.2492e-11, 2.6575e-11, 1.3510e-05, 1.6700e-05,\n",
       "            7.5800e-10, 1.8917e-11, 1.9226e-05, 6.2333e-10, 2.6045e-11, 1.2399e-10,\n",
       "            1.6532e-05, 2.1955e-10, 2.9716e-10, 1.0393e-10, 2.2903e-11, 2.1218e-11,\n",
       "            4.7150e-11, 2.3718e-11, 1.0063e-09, 2.7060e-10, 1.2180e-10, 2.4464e-12,\n",
       "            9.5469e-11, 1.7037e-10, 1.9608e-05, 2.2556e-11, 2.2710e-05, 1.0872e-10,\n",
       "            1.5798e-05, 2.8204e-11, 1.2158e-09, 5.2324e-11, 1.6828e-05, 1.9929e-11,\n",
       "            2.1513e-10, 6.0242e-10, 1.9662e-10, 7.2367e-12, 1.6472e-11, 4.0503e-11,\n",
       "            1.7981e-05, 4.3888e-11, 2.1622e-05, 5.7841e-11, 2.3479e-10, 5.1334e-12,\n",
       "            8.2822e-14, 6.0424e-12, 7.0232e-11, 6.0652e-12, 3.9158e-10, 3.3730e-12,\n",
       "            1.6748e-10, 1.3865e-10, 4.7947e-10, 4.0262e-12, 2.6132e-11, 1.5675e-05,\n",
       "            1.6789e-05, 2.9524e-10, 2.0549e-05, 3.3681e-10, 1.1670e-11, 4.1150e-13,\n",
       "            5.8569e-12, 1.4544e-12, 1.3769e-10, 8.3790e-10, 4.2125e-10, 2.2313e-14,\n",
       "            1.3798e-05, 9.9761e-11, 2.2585e-12, 1.3900e-13, 4.9884e-12, 3.1645e-12,\n",
       "            6.1021e-11, 4.5777e-11, 1.5683e-05, 1.6281e-05, 5.5707e-10, 3.4140e-10,\n",
       "            1.2145e-10, 4.2486e-11, 2.9711e-10, 4.5795e-12, 2.0076e-10, 1.6122e-05,\n",
       "            1.4501e-05, 1.4051e-10], device='cuda:0')},\n",
       "   140257463675568: {'step': 7175,\n",
       "    'exp_avg': tensor([[ 5.6052e-45, -5.6052e-45,  5.6052e-45,  ...,  4.1852e-04,\n",
       "              4.0991e-03,  5.6052e-45],\n",
       "            [ 5.6052e-45, -5.6052e-45,  5.6052e-45,  ...,  1.7118e-07,\n",
       "              4.1645e-04, -5.6052e-45],\n",
       "            [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ...,  6.5819e-06,\n",
       "              2.3782e-04,  5.6052e-45],\n",
       "            ...,\n",
       "            [ 5.6052e-45, -5.6052e-45,  5.6052e-45,  ..., -5.3580e-03,\n",
       "             -2.2141e-02,  5.6052e-45],\n",
       "            [ 5.6052e-45, -5.6052e-45,  5.6052e-45,  ..., -1.3751e-03,\n",
       "             -1.4262e-03, -5.6052e-45],\n",
       "            [-5.6052e-45,  5.6052e-45,  5.6052e-45,  ...,  9.8031e-04,\n",
       "             -2.5853e-04,  5.6052e-45]], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([[6.0084e-12, 1.7851e-08, 2.2400e-11,  ..., 5.8353e-04, 1.6189e-03,\n",
       "             3.8659e-11],\n",
       "            [3.1611e-12, 4.9662e-08, 1.4375e-11,  ..., 1.0414e-05, 1.1698e-05,\n",
       "             6.7128e-11],\n",
       "            [1.9858e-11, 2.8886e-08, 8.8118e-11,  ..., 1.5863e-05, 4.8161e-04,\n",
       "             6.5026e-11],\n",
       "            ...,\n",
       "            [5.0194e-11, 2.7748e-08, 1.0316e-10,  ..., 1.8851e-03, 5.6230e-03,\n",
       "             7.9006e-11],\n",
       "            [1.1896e-12, 3.6737e-08, 3.9236e-11,  ..., 1.9660e-03, 1.2282e-03,\n",
       "             3.3505e-10],\n",
       "            [9.4576e-10, 4.2871e-08, 3.5886e-10,  ..., 3.7927e-04, 2.2089e-04,\n",
       "             1.3641e-10]], device='cuda:0')},\n",
       "   140257464123752: {'step': 7175,\n",
       "    'exp_avg': tensor([ 3.7828e-04,  2.3444e-04,  5.1359e-05, -1.2978e-03, -1.7299e-05,\n",
       "             1.1067e-03,  5.2396e-04,  1.8536e-04, -2.8406e-05,  2.7450e-07,\n",
       "            -2.8356e-04,  2.7060e-05, -1.6045e-03,  8.9706e-04,  2.7248e-05,\n",
       "            -4.5981e-04, -2.8434e-05,  3.3611e-04, -1.6504e-03,  4.2978e-04,\n",
       "             4.7040e-04,  1.2665e-03,  5.0173e-05, -4.1383e-04,  5.1448e-05,\n",
       "            -5.7193e-04, -3.8530e-04, -1.5588e-03,  1.7041e-04, -2.5528e-06,\n",
       "            -2.2007e-05, -2.4403e-04,  5.2140e-04,  1.0808e-03, -4.3590e-05,\n",
       "             1.3704e-03, -2.0254e-04,  3.9756e-03, -1.1376e-04,  1.1696e-04,\n",
       "             5.0006e-03, -2.5422e-03,  8.6752e-05,  2.2562e-04, -1.7725e-06,\n",
       "            -2.1541e-04,  2.7146e-06, -1.6571e-05, -4.2588e-04,  3.6931e-03,\n",
       "            -2.1982e-05,  5.8460e-04,  5.9420e-05,  3.4973e-04, -3.5999e-05,\n",
       "            -3.7185e-04, -9.0493e-05, -2.1916e-03,  1.8461e-04,  7.0462e-07,\n",
       "            -4.3489e-05, -2.8047e-04,  2.2824e-05, -4.3340e-06,  1.1785e-05,\n",
       "            -7.3233e-05,  6.6583e-04, -2.8141e-05,  2.7695e-05, -3.0455e-05,\n",
       "            -4.6824e-05,  2.1906e-03, -3.6530e-03, -2.6376e-03,  3.1250e-03,\n",
       "            -5.6074e-04,  6.6627e-04,  5.1753e-05,  4.7365e-04, -2.4472e-06,\n",
       "             3.0794e-04,  9.6281e-05,  6.1292e-05, -4.9306e-05,  9.6469e-05,\n",
       "            -3.0638e-04, -9.0065e-04, -4.3584e-04, -1.0963e-03, -1.6079e-03,\n",
       "             6.1994e-04, -3.9110e-04, -9.3154e-04,  1.8914e-03,  1.3090e-04,\n",
       "             2.3277e-04, -1.4596e-04, -8.0349e-05, -2.4135e-03, -2.9800e-03,\n",
       "            -9.3142e-04,  3.4142e-04], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([1.8878e-05, 7.9716e-06, 5.8590e-06, 3.9624e-05, 2.2750e-06, 4.7652e-05,\n",
       "            2.2245e-05, 9.7345e-06, 1.0684e-05, 1.3816e-05, 2.3145e-05, 9.5993e-06,\n",
       "            3.8338e-05, 2.3718e-05, 6.5689e-06, 2.0518e-05, 1.4103e-05, 2.0412e-05,\n",
       "            1.0563e-05, 2.7665e-05, 4.7485e-06, 1.5765e-05, 6.5040e-06, 2.0485e-05,\n",
       "            1.8554e-05, 5.8488e-05, 4.4134e-05, 3.9680e-05, 3.0738e-05, 1.4115e-05,\n",
       "            4.7744e-06, 8.8474e-06, 3.5257e-05, 1.2949e-05, 1.1873e-05, 3.9167e-05,\n",
       "            6.8591e-05, 4.6693e-05, 1.9829e-05, 2.4877e-05, 1.0743e-04, 1.5725e-05,\n",
       "            2.7825e-05, 3.3714e-05, 3.3049e-06, 1.9532e-05, 5.3002e-06, 1.6960e-05,\n",
       "            1.8699e-05, 1.0472e-04, 1.6714e-05, 3.0522e-05, 9.4499e-06, 2.8292e-05,\n",
       "            8.4814e-06, 1.5342e-05, 6.8012e-06, 6.9988e-06, 2.7856e-06, 2.3613e-06,\n",
       "            7.2009e-06, 2.3678e-05, 2.4924e-06, 2.3895e-06, 1.2854e-05, 2.4646e-06,\n",
       "            2.3201e-05, 3.3269e-05, 1.1952e-05, 1.2502e-05, 1.2060e-05, 6.2513e-06,\n",
       "            7.5464e-05, 4.5817e-05, 8.0050e-05, 2.9274e-05, 3.2670e-05, 2.4059e-05,\n",
       "            4.1161e-05, 4.4037e-06, 8.1695e-06, 2.1860e-05, 1.2069e-05, 5.9490e-05,\n",
       "            6.4735e-05, 6.0471e-05, 2.4285e-05, 1.3623e-05, 3.0924e-05, 6.7633e-05,\n",
       "            5.4147e-05, 2.3758e-05, 6.3520e-05, 2.8298e-05, 2.3318e-05, 1.8644e-05,\n",
       "            2.8240e-05, 7.6522e-05, 8.8149e-05, 7.9229e-05, 4.2329e-05, 1.0971e-05],\n",
       "           device='cuda:0')}},\n",
       "  'param_groups': [{'lr': 0.001,\n",
       "    'betas': (0.9, 0.999),\n",
       "    'eps': 1e-08,\n",
       "    'weight_decay': 0,\n",
       "    'amsgrad': False,\n",
       "    'params': [140255695940488,\n",
       "     140255695940128,\n",
       "     140257463675568,\n",
       "     140257464123752]}]})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DONE: Write a function that loads a checkpoint and rebuilds the model\n",
    "def load_checkpoint(path):\n",
    "    \n",
    "    checkpoint = torch.load(path)#loads entire checkpoint as a dictionary\n",
    "    model = checkpoint['model']#captures the model with weights and biases\n",
    "    optimizer = checkpoint['optimizer_state']#captures the optimizer and its parameters \n",
    "    model.class_to_idx = checkpoint['class_to_idx']\n",
    "    \n",
    "    return checkpoint, optimizer, model.class_to_idx\n",
    "\n",
    "checkpoint, optimizer, class_to_idx = load_checkpoint('checkpoint.pth')\n",
    "checkpoint, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference for classification\n",
    "\n",
    "Now you'll write a function to use a trained network for inference. That is, you'll pass an image into the network and predict the class of the flower in the image. Write a function called `predict` that takes an image and a model, then returns the top $K$ most likely classes along with the probabilities. It should look like \n",
    "\n",
    "```python\n",
    "probs, classes = predict(image_path, model)\n",
    "print(probs)\n",
    "print(classes)\n",
    "> [ 0.01558163  0.01541934  0.01452626  0.01443549  0.01407339]\n",
    "> ['70', '3', '45', '62', '55']\n",
    "```\n",
    "\n",
    "First you'll need to handle processing the input image such that it can be used in your network. \n",
    "\n",
    "## Image Preprocessing\n",
    "\n",
    "You'll want to use `PIL` to load the image ([documentation](https://pillow.readthedocs.io/en/latest/reference/Image.html)). It's best to write a function that preprocesses the image so it can be used as input for the model. This function should process the images in the same manner used for training. \n",
    "\n",
    "First, resize the images where the shortest side is 256 pixels, keeping the aspect ratio. This can be done with the [`thumbnail`](http://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.thumbnail) or [`resize`](http://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.thumbnail) methods. Then you'll need to crop out the center 224x224 portion of the image.\n",
    "\n",
    "Color channels of images are typically encoded as integers 0-255, but the model expected floats 0-1. You'll need to convert the values. It's easiest with a Numpy array, which you can get from a PIL image like so `np_image = np.array(pil_image)`.\n",
    "\n",
    "As before, the network expects the images to be normalized in a specific way. For the means, it's `[0.485, 0.456, 0.406]` and for the standard deviations `[0.229, 0.224, 0.225]`. You'll want to subtract the means from each color channel, then divide by the standard deviation. \n",
    "\n",
    "And finally, PyTorch expects the color channel to be the first dimension but it's the third dimension in the PIL image and Numpy array. You can reorder dimensions using [`ndarray.transpose`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ndarray.transpose.html). The color channel needs to be first and retain the order of the other two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://stackoverflow.com/questions/16646183/crop-an-image-in-the-centre-using-pil for center cropping images to 224X224\n",
    "I could have used the transforms module, but I thought it wise to also learn how to crop using PIL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pink_flower.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8e6910b5bbf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#this just tests the output on a random image picked from google images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pink_flower.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-8e6910b5bbf0>\u001b[0m in \u001b[0;36mprocess_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      4\u001b[0m     '''\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmin_leng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;31m#shortest side is 256 pixels, the minimum length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2580\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2581\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pink_flower.jpg'"
     ]
    }
   ],
   "source": [
    "def process_image(image):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "    \n",
    "    img = PIL.Image.open(image)\n",
    "    \n",
    "    min_leng = 256 #shortest side is 256 pixels, the minimum length\n",
    "    ratio = min_leng/min(img.size) #this is used to retain the aspect ratio after cropping\n",
    "    other_leng = int(max(img.size)*ratio) #this captures the longer side of the image\n",
    "    img = img.resize((min_leng,other_leng), PIL.Image.ANTIALIAS) \n",
    "    \n",
    "    #center cropping image to 224 X 224\n",
    "    left = (min_leng - 224)/2\n",
    "    top = (other_leng - 224)/2\n",
    "    right = (min_leng + 224)/2\n",
    "    bottom = (other_leng + 224)/2\n",
    "    img = img.crop((left, top, right, bottom))\n",
    "\n",
    "    #converts the PIL image to a numpy array and scales the image to range [0,1]\n",
    "    np_img = np.array(img)/np.max(img)\n",
    "    \n",
    "    #means and standard deviations of the red, green, blue channels, respectively\n",
    "    means = np.array([0.485, 0.456, 0.406])\n",
    "    stds = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    #normalizes the image\n",
    "    np_img = (np_img - means)/stds\n",
    "    \n",
    "    #transposes the image so the color channels are first for pytorch format\n",
    "    np_img = np.ndarray.transpose(np_img)\n",
    "    \n",
    "    py_tensor = torch.from_numpy(np_img) #converts the numpy array into a pytorch tensor\n",
    "    \n",
    "    return py_tensor\n",
    "\n",
    "#this just tests the output on a random image picked from google images\n",
    "img = process_image('pink_flower.jpg')\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check your work, the function below converts a PyTorch tensor and displays it in the notebook. If your `process_image` function works, running the output through this function should return the original image (except for the cropped out portions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image_path, ax=None, title=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    image = process_image(image_path)\n",
    "    image = image.numpy() #converts pytorch tensor to numpy array\n",
    "    \n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    image = image.transpose((1, 2, 0))\n",
    "    \n",
    "    # Undo preprocessing\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    \n",
    "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "imshow('flowers/test/101/image_07988.jpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Prediction\n",
    "\n",
    "Once you can get images in the correct format, it's time to write a function for making predictions with your model. A common practice is to predict the top 5 or so (usually called top-$K$) most probable classes. You'll want to calculate the class probabilities then find the $K$ largest values.\n",
    "\n",
    "To get the top $K$ largest values in a tensor use [`x.topk(k)`](http://pytorch.org/docs/master/torch.html#torch.topk). This method returns both the highest `k` probabilities and the indices of those probabilities corresponding to the classes. You need to convert from these indices to the actual class labels using `class_to_idx` which hopefully you added to the model or from an `ImageFolder` you used to load the data ([see here](#Save-the-checkpoint)). Make sure to invert the dictionary so you get a mapping from index to class as well.\n",
    "\n",
    "Again, this method should take a path to an image and a model checkpoint, then return the probabilities and classes.\n",
    "\n",
    "```python\n",
    "probs, classes = predict(image_path, model)\n",
    "print(probs)\n",
    "print(classes)\n",
    "> [ 0.01558163  0.01541934  0.01452626  0.01443549  0.01407339]\n",
    "> ['70', '3', '45', '62', '55']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, device):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    img = process_image(image_path).float()\n",
    "    img = img.unsqueeze_(0)\n",
    "    \n",
    "    checkpoint, _, class_to_idx = load_checkpoint('checkpoint.pth')\n",
    "    \n",
    "    model = checkpoint['model']\n",
    "    \n",
    "    model.eval() #turns dropout off \n",
    "    \n",
    "    with torch.no_grad(): #gradient turned off because during prediction we want to know the result from training\n",
    "        \n",
    "        output = model.forward(img.to(device))\n",
    "        probs, class_indices = F.softmax(output.data,dim=1).topk(5)\n",
    "        \n",
    "        probs = list(np.array(probs[0]))\n",
    "        \n",
    "        class_indices = list(np.array(class_indices[0]))\n",
    "        \n",
    "        classes = [k for k,v in checkpoint['class_to_idx'].items() if v in class_indices]\n",
    "        \n",
    "    return probs, classes\n",
    "\n",
    "probs, classes = predict('flowers/test/101/image_07988.jpg', 'cuda')\n",
    "print(probs)\n",
    "print(classes)\n",
    "    \n",
    "    # DONE: Implement the code to predict the class from an image file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checking\n",
    "\n",
    "Now that you can use a trained model for predictions, check to make sure it makes sense. Even if the testing accuracy is high, it's always good to check that there aren't obvious bugs. Use `matplotlib` to plot the probabilities for the top 5 classes as a bar graph, along with the input image. It should look like this:\n",
    "\n",
    "<img src='assets/inference_example.png' width=300px>\n",
    "\n",
    "You can convert from the class integer encoding to actual flower names with the `cat_to_name.json` file (should have been loaded earlier in the notebook). To show a PyTorch tensor as an image, use the `imshow` function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [15,5])\n",
    "\n",
    "#left plot\n",
    "img_path = test_dir + '/101/image_07988.jpg'\n",
    "ax1 = imshow(img_path, ax=plt.subplot(1, 2, 1))\n",
    "ax1.set_title(cat_to_name['101'])\n",
    "\n",
    "probs, classes = predict(img_path)\n",
    "class_names = [cat_to_name[i] for i in classes]\n",
    "#right plot\n",
    "plt.subplot(1,2,2)\n",
    "sb.barplot(x=probs, y = class_names, color='blue')\n",
    "plt.xlabel('Probability')\n",
    "\n",
    "plt.show();\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
